{"pageProps":{"member":{"id":"nwiizo","name":"nwiizo","role":"Software Developer","bio":"Brogrammer","avatarSrc":"/avatars/nwiizo.jpeg","sources":["https://syu-m-5151.hatenablog.com/feed","https://zenn.dev/nwiizo/feed"],"includeUrlRegex":"","twitterUsername":"nwiizo","githubUsername":"nwiizo","websiteUrl":"https://nwiizo.github.io/"},"postItems":[{"title":"Google Cloud が公開しているIP rangeから特定のRegionのIP rangeを抜き出す","contentSnippet":"やったことGoogle Cloud はhttps://www.gstatic.com/ipranges/cloud.json というサイトでGoogle Cloud で利用しているIP rangeが公開されている。余談ではありますがAWSは同様に https://ip-ranges.amazonaws.com/ip-ranges.json に公開されている。みんなが大好きなjqコマンドのselectでscopeの値がasia-northeast1 なものだけを抜き出しています。また、サービスによって自分の利用しているリージョン以外へのアクセスが必要なものもあると思うのでアクセスリストを作る際には気をつけてほしいです。curl -s https://www.gstatic.com/ipranges/cloud.json | jq -r '.prefixes[] | select(.scope == \"asia-northeast1\") | .ipv4Prefix' | grep -v null結果(asia-northeast1)で利用されているIPアドレス34.84.0.0/1634.85.0.0/1734.104.62.0/2334.104.128.0/1734.127.190.0/2334.146.0.0/1634.157.64.0/2034.157.164.0/2234.157.192.0/2035.187.192.0/1935.189.128.0/1935.190.224.0/2035.194.96.0/1935.200.0.0/1735.213.0.0/1735.220.56.0/2235.221.64.0/1835.230.240.0/2035.242.56.0/2235.243.64.0/18104.198.80.0/20104.198.112.0/20参考GCPのIPアドレス範囲をリスト化AWSとかGCPが公式に公開されているIP rangeを取得するツールを書いた - 地方エンジニアの学習日記","link":"https://syu-m-5151.hatenablog.com/entry/2022/08/24/115833","isoDate":"2022-08-24T02:58:33.000Z","dateMiliSeconds":1661309913000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"2022年版 OpenTelemetryを知れば世界が平和に","contentSnippet":"はじめにOpenTelemetryとはOpentelemetry のコンポーネントOpentelemetry のプロジェクトの仕様とStatusTracingMetricsLogging(Specification にドキュメントがない)BaggageOpenTelemetry のSpanとTraceOpenTelemetry CollectorとはCollector のメリットOpenTelemetry Collector Architecture とはOpenTelemetry とSDKとパッケージOpenTelemetry と自動計装今後のOpentelemetry について次回予告:OpenTelemetry とOpenTelemetry Collectorを使ったTracingとMetricsをアプリケーションで利用する方法参照リンクはじめに最初は、SRE に成る君に最低限の開発力を身に着けてほしい の解像度の上げる方法の為のGo言語みたいなこと書こうしました。しかし、内容をまとめる能力が乏しく、断念しました。書いている途中で、参考資料として改訂2版 みんなのGo言語 や実用 Go言語、Cloud Native Go を読んでいたら、この内容を齟齬なく伝える自信が完全になくなったので読んでもらえばいいやと自暴自棄になりました。今回は、OpenTelemetry の実装や仕組みに対する言及などが社内でないような気がしました。この共有会ではOpenTelemetry の話を何となく聞かれた時に答えられるようにしていただければと思います。OpenTelemetryプロジェクトを実際に使わなくてもプロジェクトの状況や概念、項目(ログの設計など)を知ることで現在の監視設計や運用、判断に活かせる場面が出てくるかもしれません。現状のOpenTelemetry はログ、メトリクス、トレースの全てカバーできるツールというわけではないですが1年後にはその全てをサポートしてそうな勢いと計画があります。OpenTelemetryとはhttps://opentelemetry.io/docs/concepts/what-is-opentelemetry/OpenTelemetry は、オブザーバビリティの三本柱のログ、メトリクス、トレースの計装と収集を標準化しようとする野心的なプロジェクトです。ベンダーに依存しない実装を提供し、選択したバックエンドにテレメトリーデータを送信する方法を標準化することを目的としています。OpenTracingとOpenCensusの後継的なプロジェクトで新たな標準化ツールとなります。また、仕様が全体的に固まってきたので今後、テレメトリデータを扱うツールとして広まっていくのではないかと思います。 こちらはOpenTelemetryのこれまでとこれから の資料を参考にさせていただきました。💡 Telemetry とは遠隔測定法（えんかくそくていほう）は、観測対象から離れた地点から様々な観測を行い、そのデータを取得する技術です。観測地点に常駐することが物理的・経済的あるいは安全上困難な場合や、観測対象が移動する場合に使用されます。テレメトリー(telemetry) あるいはテレメタリング(telemetering) ということもあります。 装置そのものは、テレメータ (telemeter) と呼ばれています。https://ja.wikipedia.org/wiki/遠隔測定法Opentelemetry のコンポーネントhttps://opentelemetry.io/docs/concepts/components/OpenTelemetryは現在、いくつかの主要コンポーネントで構成されています。仕様すべての実装に対する言語に限らない横断的な要件と実装に必要な事項を記述する。CollectorOpenTelemetry Collectorは、テレメトリデータを受信、処理、およびエクスポートできるベンダーに依存しないプロキシです。複数の形式（OTLP、Jaeger、Prometheus、および多くの商用/独自のツールなど）でのテレメトリデータの受信と、1つ以上のバックエンドへのデータの送信をサポートします。自動計装OpenTelemetry は、サポートされる言語用の一般的なライブラリやフレームワークから関連するテレメトリデータを生成する幅広い数のコンポーネントをサポートします。例えば、HTTP ライブラリからのインバウンドとアウトバウンドの HTTP リクエストは、それらのリクエストに関するデータを生成します。自動計測の使用方法は言語によって異なり、アプリケーションと一緒にロードするコンポーネントの使用を好むか要求するかもしれませんし、コードベースで明示的にパッケージを取り込むのが良いかもしれません。言語ごとのSDKOpenTelemetryには言語SDKもあります。OpenTelemetry APIを使用して、選択した言語でテレメトリデータを生成し、そのデータを優先バックエンドにエクスポートすることもできます。これらのSDKを使用すると、アプリケーションの手動インストルメンテーションに接続するために使用できる一般的なライブラリおよびフレームワークの自動インストルメンテーションを組み込むこともできます。ベンダーは、バックエンドへのエクスポートを簡単にするために、言語SDKの配布を行うことがよくあります。Opentelemetry のプロジェクトの仕様とStatushttps://opentelemetry.io/status/現在、皆さんが関わっている各案件でOpenTelemetry の利用が可能かどうかについて聞かれると思います。何となく聞かれた時に答えられるようにこれ、各シグナルごとにこれぐらいは進んでいるんだと理解していただければと思います。OpenTelemetryは、シグナルごとに開発されています。シグナルとはトレース、メトリクス、バッゲージ、ロギングなどの仕様でサポートされているテレメトリのカテゴリを指しています。シグナルは、分散システム間でデータを相関させるための共有メカニズムのcontext propagation(コンテキストの伝播)の上に構築されています。これらは主に4つで構成されています。Tracinghttps://opentelemetry.io/docs/concepts/signals/traces/API: stable, feature-freezeSDK: stableProtocol: stableNotes:トレース仕様は現在完全に安定しており、長期的なサポートでカバーされています。トレース仕様はまだ拡張可能ですが、後方互換性のある方法でのみ行われます。OpenTelemetryクライアントは、そのトレース実装が完了した時点で、v1.0にバージョンアップされます。Metricshttps://opentelemetry.io/docs/concepts/signals/metrics/API: stableSDK: mixedProtocol: stableNotes:OpenTelemetry Metricsは現在活発に開発中です。データモデルは安定しており、OTLPプロトコルの一部としてリリースされています。メトリックパイプラインの実験的なサポートはCollectorで利用可能です。PrometheusのCollectorサポートは、Prometheusコミュニティと協力して、現在開発中です。Logging(Specification にドキュメントがない)https://opentelemetry.io/docs/concepts/signals/logs/API: draftSDK: draftProtocol: stableNotes:OpenTelemetry Logging は現在、活発に開発が進められています。ログデータモデルはOpenTelemetryプロトコルの一部としてリリースされています。OpenTelemetry プロジェクトへの Stanza の寄贈により、多くのデータフォーマットに対するログ処理が Collector に追加されています。現在、多くの言語でのログアペンダが開発中です。ログ・アペンダーは、トレースやスパンIDなどのOpenTelemetryトレース・データを既存のロギング・システムに付加することができます。OpenTelemetry ロギングSDKも現在開発中です。これにより、OpenTelemetryクライアントが既存のロギングシステムからロギングデータを取り込み、トレースやメトリクスとともにOTLPの一部としてログを出力することができます。OpenTelemetryのロギングAPIは、現在開発中ではありません。まず、既存のロギングシステムとの統合に重点を置いています。メトリクスが完成したら、OpenTelemetryのロギングAPIの開発に焦点を移します。Baggagehttps://opentelemetry.io/docs/concepts/signals/baggage/API: stable, feature-freezeSDK: stableProtocol: N/ANotes:OpenTelemetry Baggage は現在完全に安定しています。Baggage は観測可能なツールではなく、トランザクションに任意のキーと値を付加し、下流のサービスがそれらにアクセスできるようにするためのシステムです。そのため、BaggageにはOTLPやCollectorのコンポーネントはありません。OpenTelemetry のSpanとTracehttps://opentelemetry.io/docs/concepts/observability-primer/OpenTelemetryにおけるトレース情報はSpanとTraceという概念で定義されています。Span: リクエスト内の各処理の情報（e.g. 処理名、実行時間、ステータスコードなどなど）Trace: あるリクエストに対するSpanのまとまりhttps://lightstep.com/opentelemetry/spans  より画像の引用Span はトレースの構成要素で、いくつかの情報を持ちます。複数のスパンをつなぎ合わせて、Trace を作成します。Trace は、多くの場合、各Span が開始および完了した時間を反映するSpan の「Tree」と見なされます。また、Span 間の関係も示します。Span の目的は、プログラムの実行に関する情報を観測可能なツールに提供することです。詳細が含まれている必要があり、Trace は全体像を把握するために必要な情報が含まれます。Loggingでは今後、トレースとリンクできるようにトレースIDを持つようになることが検討されてるようです。個々の情報を含むSpanName名前Start and End Timestamps終了と開始の時間Span ContextSpan Context は、Trace ID と Span IDを提供します。各Span は、Span ID と呼ばれるTrace 内で一意の ID によって識別されます。Span はTraceIDを使用して、Spanとそのトレース間の関係を識別します。Span は、サービスやプロセスの境界を越えて移動するために、Span Contextを必要とします。ログに含めることでログとSpan を紐付けることもできます。Attributesメタデータを含むキーと値のペアのことで、Spanにアノテーションを付けて、追跡している操作に関する情報を運ぶために使用します。Span EventsSpan Event は通常、Spanの期間中の重要で特異な時点を示すために使用されます。Span LinksSpan Links はオプションですが相互に関連付ける為に利用されます。Span StatusステータスはSpanに添付されます。通常、アプリケーションコードに例外などの既知のエラーがある場合は、Span Statusを設定します。Sample Span:        {          \"trace_id\": \"7bba9f33312b3dbb8b2c2c62bb7abe2d\",          \"parent_id\": \"\",          \"span_id\": \"086e83747d0e381e\",          \"name\": \"/v1/sys/health\",          \"start_time\": \"2021-10-22 16:04:01.209458162 +0000 UTC\",          \"end_time\": \"2021-10-22 16:04:01.209514132 +0000 UTC\",          \"status_code\": \"STATUS_CODE_OK\",          \"status_message\": \"\",          \"attributes\": {            \"net.transport\": \"IP.TCP\",            \"net.peer.ip\": \"172.17.0.1\",            \"net.peer.port\": \"51820\",            \"net.host.ip\": \"10.177.2.152\",            \"net.host.port\": \"26040\",            \"http.method\": \"GET\",            \"http.target\": \"/v1/sys/health\",            \"http.server_name\": \"mortar-gateway\",            \"http.route\": \"/v1/sys/health\",            \"http.user_agent\": \"Consul Health Check\",            \"http.Scheme\": \"http\",            \"http.host\": \"10.177.2.152:26040\",            \"http.flavor\": \"1.1\"          },          \"events\": {            \"name\": \"\",            \"message\": \"OK\",            \"タイムスタンプ\": \"2021-10-22 16:04:01.209512872 +0000 UTC\"          }        }全体像を示すTrace  OpenTelemetry のトレースがどのように機能するかを理解するために、コードの計装に関与するコンポーネントのリストを見てみましょう。TracerTracer は、サービス内のリクエストなど、与えられた操作で何が起こっているかについての詳細情報を含むSpanを作成します。Tracer はTracer Provider から作成されます。Tracer ProviderTracer Provider（TracerProviderと呼ばれることもあります）は、Tracerを生成します。ほとんどのアプリケーションでは、Tracer Provider は一度初期化され、そのライフサイクルはアプリケーションのライフサイクルと一致します。Tracer Providerの初期化には、ResourceとExporterの初期化も含まれます。Trace ExporterTrace Exportersは、Trace をコンシューマーに送信します。このconsumer は、デバッグおよび開発時の標準出力、OpenTelemetry Collector、または任意のオープンソースまたはベンダーのバックエンドにすることができます。Trace Contextトレースコンテキストは、トレーススパンに関するメタデータで、サービスやプロセスの境界を越えてスパン間の相関関係を提供します。例えば、サービス A がサービス B を呼び出し、その呼び出しをトレースで追跡したいとします。この場合、OpenTelemetry はトレースコンテキストを使用して、サービス A からトレースの ID と現在のスパンを取得し、サービス B で作成されたスパンがトレースに接続し追加することができるようにします。これは、Context Propagation（コンテキスト伝播）と呼ばれています。Sample Trace        {            \"name\": \"Hello-Greetings\",            \"context\": {                \"trace_id\": \"0×5b8aa5a2d2c872e8321cf37308d69df2\",                \"span_id\": \"0×5fb397be34d26b51\",            },            \"parent_id\": \"0×051581bf3cb55c13\",            \"start_time\": \"2022-04-29T18:52:58.114304Z\",            \"end_time\": \"2022-04-29T18:52:58.114435Z\",            \"attributes\": {                \"http.route\": \"some_route1\"            },            \"events\": [                {                    \"name\": \"hey there!\",                    \"タイムスタンプ\": \"2022-04-29T18:52:58.114561Z\",                    \"attributes\": {                        \"event_attributes\": 1                    }                },                {                    \"name\": \"bye now!\",                    \"タイムスタンプ\": \"2022-04-29T22:52:58.114561Z\",                    \"attributes\": {                        \"event_attributes\": 1                    }                }            ],        }        {            \"name\": \"Hello-Salutations\",            \"context\": {                \"trace_id\": \"0×5b8aa5a2d2c872e8321cf37308d69df2\",                \"span_id\": \"0×93564f51e1abe1c2\",            },            \"parent_id\": \"0×051581bf3cb55c13\",            \"start_time\": \"2022-04-29T18:52:58.114492Z\",            \"end_time\": \"2022-04-29T18:52:58.114631Z\",            \"attributes\": {                \"http.route\": \"some_route2\"            },            \"events\": [                {                    \"name\": \"hey there!\",                    \"タイムスタンプ\": \"2022-04-29T18:52:58.114561Z\",                    \"attributes\": {                        \"event_attributes\": 1                    }                }            ],        }        {            \"name\": \"Hello\",            \"context\": {                \"trace_id\": \"0×5b8aa5a2d2c872e8321cf37308d69df2\",                \"span_id\": \"0×051581bf3cb55c13\",            },            \"parent_id\": null,            \"start_time\": \"2022-04-29T18:52:58.114201Z\",            \"end_time\": \"2022-04-29T18:52:58.114687Z\",            \"attributes\": {                \"http.route\": \"some_route3\"            },            \"events\": [                {                    \"name\": \"Guten Tag!\",                    \"タイムスタンプ\": \"2022-04-29T18:52:58.114561Z\",                    \"attributes\": {                        \"event_attributes\": 1                    }                }            ],        }💡 そのサービスを開発した開発者や初期から参加しているメンバーであれば、特定のリクエストが各サービス(関数)を利用しているのかある程度は把握していると思います。しかし、案件に途中で入ったり、運用をしていくエンジニアからすればそれがどのように繋がっているかなどの情報はドキュメントやログ、実際に実装を見ていく以外に方法がありません。あればあるで嬉しいが無いならないでなんとかなるのもトレーシングが普及しない問題点の一つではないかと邪推しておく。OpenTelemetry Collectorとはhttps://opentelemetry.io/docs/collector/OpenTelemetry Collector は、テレメトリデータの受信、処理、エクスポートの方法について、ベンダーに依存しない実装を提供します。OpenTelemetry Collector は、計装と収集を標準化でいうところの収集を主に担当します。アプリケーションとテレメトリデータの中継役として動作するため各ベンダー固有のテレメトリデータをバックエンド(Jaeger、Prometheus、Fluent Bitなど)の対応したデータ形式に変換したりといった役割を持ちます。そのため、複数のエージェント/コレクタを実行、操作、保守する必要がなくなります。Collector のメリット使いやすさ: デフォルトの設定を用意、よくあるプロトコルのサポート、そのまま使えるパフォーマンス: さまざまな負荷や構成に対応できるように設定することもできますオブザーバビリティ: それ自体が観測可能である拡張性: コアコードに手を入れることなく拡張が可能統合: 単一のコード、エージェントでまたはコレクターとして配置可能でトレース、メトリック、ログ(将来)を扱うOpenTelemetry Collectorは、AgentとGatewayの2つのデプロイメント方法から選ぶことができます。Agent: アプリケーションとともに、またはアプリケーションと同じホストで実行されるCollector インスタンス(バイナリ、サイドカー、デーモンセットなど)Gateway: 通常、クラスタ、データセンター、地域ごとに単独のサービス（コンテナやデプロイメントなど）として稼働する1つまたは複数のCollectorインスタンス。OpenTelemetry Collector Architecture とはhttps://github.com/open-telemetry/opentelemetry-collector/blob/main/docs/design.mdOpenTelemetry Collector Architectureは大まかにReceiver、Processor、Exporterの3つの要素で構成されいます。Receiversどういうフォーマットでどのようにテレメトリデータを受信するかという設定サードパーティのテレメトリデータを受け取って、内部的にTraceとSpanに変換する役割を持つProcessorsテレメトリデータの加工、フィルター、リトライ、バッチ処理等の設定Receiverから送られてきたTraceとSpan情報を特定の条件で加工するExportersテレメトリデータのエクポートに関する設定Processorから送られてきたデータを、Export先のデータ形式に変換し、送信するOpenTelemetry Collector の設定ファイル    receivers:      otlp:        protocols:          grpc:          http:      otlp/2:        protocols:          grpc:            endpoint: 0.0.0.0:55690        processors:      batch:      batch/test:        exporters:      otlp:        endpoint: otelcol:4317      otlp/2:        endpoint: otelcol2:4317        extensions:      health_check:      pprof:      zpages:        service:      extensions: [health_check,pprof,zpages]      pipelines:        traces:          receivers: [otlp]          processors: [batch]          exporters: [otlp]        traces/2:          receivers: [otlp/2]          processors: [batch/test]          exporters: [otlp/2]        metrics:          receivers: [otlp]          processors: [batch]          exporters: [otlp]        logs:          receivers: [otlp]          processors: [batch]          exporters: [otlp]OpenTelemetry とSDKとパッケージhttps://opentelemetry.io/docs/concepts/instrumenting/OpenTelemetryプロジェクトは、各言語ごとにテレメトリデータを送信するパッケージが用意されています。これらはアプリケーションの計測を容易にします。インストルメンテーションライブラリは、言語ごとにコアリポジトリを提供します。自動計測または非コアコンポーネント用の追加のリポジトリを提供する場合と提供しない場合があります。opentelemetry-goTracing はStableGoのコアリポジトリでありテレメトリデータ作成機能やJaeger、Zipkinといった主要なOSSやOTLPにテレメトリデータをexportするための機能を提供しています。Metrics はAlphaメトリクスの機能はまだAlpha提供でいくつかのIssue は積まれている状態  OpenTelemetry Go: Metric SDK * open-telemetryLogging はFrozenTracing のMetrics 2つの機能開発をやっているのでそれらが終わるまではIssue を認めないがopentelemetry-go-contribコア機能でないものや、その他のオープンソースや商用のバックエンドのための実装を含みます。Go言語の場合にここに計装系のコードも含まれているOpenTelemetry と自動計装https://opentelemetry.io/docs/concepts/instrumenting-library/OpenTelemetryプロジェクトは、各言語ごとにテレメトリデータを送信するパッケージが用意されていますが、ライブラリーによっては全てを実装しなくてもよいことがあります。全てとはOpenTelemetry Trace は以下のような手順で作成されます。これらを全て自分でやるのは流石に骨が折れる作業です。1. Exporter 作成2. TracerProvider作成3. Tracer取得4. Span作成上記の手順を生のAPIを叩いても実施してもよいのですが、アプリケーションの特定のミドルウェアやフレームワークとのインタフェースがinstrumentationとして提供されており、2~4 を自動で取得することができます。トレース情報を取り出す便利ライブラリがいくつもあります(トレース、メトリクス、ロギングの全てが自動で取得できる世界線までもう少し)。📝 Registry はOpenTelemetry で利用されるライブラリ、プラグイン、およびその他の便利なツールを確認することができるので確認など確認してみると自分で利用したいツールなどが見つかるかもしれません。https://opentelemetry.io/registry/OpenTelemetryとhttptrace.ClientTraceを使ってHTTPリクエストのlatencyを可視化する を参考にコードを作成しました。OpenTelemetry Collector は使用せずにJaeger のエンドポイントを叩いてる    package main        import (      \"context\"      \"log\"      \"net/http\"      \"net/http/httptrace\"          _ \"go.opencensus.io/resource\"      _ \"go.opencensus.io/trace\"      \"go.opentelemetry.io/otel/attribute\"      \"go.opentelemetry.io/otel/exporters/jaeger\"      \"go.opentelemetry.io/otel/sdk/resource\"      \"go.opentelemetry.io/otel/sdk/trace\"      semconv \"go.opentelemetry.io/otel/semconv/v1.10.0\"          \"go.opentelemetry.io/contrib/instrumentation/net/http/httptrace/otelhttptrace\"      \"go.opentelemetry.io/otel\"    )        func main() {      tracerProvider, err := NewTracerProvider(\"otelhttp_client_trace\")      if err != nil {          log.Fatal(err)      }      defer func() {          if err := tracerProvider.Shutdown(context.Background()); err != nil {              log.Fatal(err)          }      }()      otel.SetTracerProvider(tracerProvider)          ctx := context.Background()      ctx, span := tracerProvider.Tracer(\"main\").Start(ctx, \"main\")      defer span.End()          if err := httpGet(ctx, \"https://3-shake.com/\"); err != nil {          log.Fatal(err)      }    }        func httpGet(ctx context.Context, url string) error {      ctx, span := otel.Tracer(\"main\").Start(ctx, \"httpGet\")      defer span.End()      span.SetAttributes(attribute.Key(\"url\").String(url))          clientTrace := otelhttptrace.NewClientTrace(ctx)      ctx = httptrace.WithClientTrace(ctx, clientTrace)      req, err := http.NewRequestWithContext(ctx, \"GET\", url, nil)      if err != nil {          return err      }      _, err = http.DefaultClient.Do(req)      if err != nil {          return err      }      return nil    }        func NewTracerProvider(serviceName string) (*trace.TracerProvider, error) {      // Port details: https://www.jaegertracing.io/docs/getting-started/      collectorEndpointURI := \"http://localhost:14268/api/traces\"          exporter, err := jaeger.New(jaeger.WithCollectorEndpoint(jaeger.WithEndpoint(collectorEndpointURI)))      if err != nil {          return nil, err      }          r := NewResource(serviceName, \"v1\", \"local\")      return trace.NewTracerProvider(          trace.WithBatcher(exporter),          trace.WithResource(r),          trace.WithSampler(trace.TraceIDRatioBased(1)),      ), nil    }        func NewResource(serviceName string, version string, environment string) *resource.Resource {      r, _ := resource.Merge(          resource.Default(),          resource.NewWithAttributes(              semconv.SchemaURL,              semconv.ServiceNameKey.String(serviceName),              semconv.ServiceVersionKey.String(version),              attribute.String(\"environment\", environment),          ),      )      return r    }それらをjaeger (イエーガー)に食べさせた結果がこれ今後のOpentelemetry についてhttps://www.cncf.io/blog/2022/07/07/opentelemetry-roadmap-and-latest-updates/各ライブラリーでの対応は別としてこのような発言もあります。Realistically at this point in time, I don’t expect logging to be stable until the end of the year at the earliest but I’d say early next year.現実的に現時点では、ロギングが安定するのは早くても年末ですが、来年の早いタイミングだとは思います。-> OpenTelemetry  を取り巻く環境は来年2023年4月ぐらいにもう一度取り扱いと思います。他にも以下のようなトピックに触れておりました。OpenTelemetryがMetricsのRCに到達.Logs の仕様が安定、Logs Beta の予定.OpenTelemetryへのリアルユーザーモニタリングの追加.OpenTelemetryへの継続的プロファイリングの追加.リモートエージェント管理による操作性の向上.OpenTelemetryに4317番ポートが登録.eBPFとその他のアップデート.とても気になるトピックが多くありますがこの記事では紹介しません。次回予告:OpenTelemetry とOpenTelemetry Collectorを使ったTracingとMetricsをアプリケーションで利用する方法ざっくりとではありましたがOpenTelemetryに関する技術要素とその概要をまとめました。野心的なプロジェクトでまだ道半ばですが個人的には将来がとても楽しみです。OpenTelemetry とOpenTelemetry Collectorを使ったTracingとMetricsをアプリケーションで利用する場合、基本的な流れは次のようになります。# Tracing 1. Exporter 作成2. TracerProvider作成3. Tracer取得4. Span作成# Metrics1. Exporter 作成2. MeterProvider　作成3. Meter 作成4. Instrument　作成5. Measurement 作成上記に関しては社内ハンズオンなどで実施していきたいと思います。社内共有会で利用したものだからといって公開するにあたって参照を記載する際に気をつけることを学びました。良い学びです。参照リンクOpenTelemetryhttps://opentelemetry.io/ OpenTelemetry roadmap and latest updates https://www.cncf.io/blog/2022/07/07/opentelemetry-roadmap-and-latest-updates/ Spans in OpenTelemetry https://lightstep.com/opentelemetry/spansOpenTelemetryのこれまでとこれからhttps://event.cloudnativedays.jp/o11y2022/talks/1347入門 OpenTelemetry Collectorhttps://event.cloudnativedays.jp/o11y2022/talks/1354OpenTelemetryとgo-chiを繋げてみるhttps://future-architect.github.io/articles/20211020a/ OpenTelemetryとhttptrace.ClientTraceを使ってHTTPリクエストのlatencyを可視化する https://journal.lampetty.net/entry/opentelemetry-httptrace AWS Distro for OpenTelemetry https://aws.amazon.com/jp/otel/ Go と OpenTelemetry https://cloud.google.com/trace/docs/setup/go-ot","link":"https://syu-m-5151.hatenablog.com/entry/2022/07/12/115434","isoDate":"2022-07-12T02:54:34.000Z","dateMiliSeconds":1657594474000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"SRE に成る君に最低限の開発力を身に着けてほしい","contentSnippet":"はじめにまず、はじめに皆さんへ言っておきたいことがあります。このドキュメントの目的は皆さんをやる気にさせて一心不乱にコードを書きまくって新機能追加や改善をしてソフトウェアを開発していってほしいというわけではないということです。もちろん、そうなってくれれば嬉しいですが気合が入ったからプログラムを急に書けるようになるわけではないのでそのような目的は一切ありません。また、この文章にはインフラエンジニアがコードを読み書きできなくて良いという意図はなくポジショニングトーク的にSREという単語を利用しておりますので何も言わないでください。SREはそもそも、コードを書かなくてもよいエンジニアではないSREとは、ITサービスの信頼性を高めるために、ITエンジニア（開発者）が信頼性向上のために行う設計やアプローチ、またはこれらを行うチームや役割を指します。Google では、SREチームの50～60%は「Google のソフトウェアエンジニア」で、残りの40～50%は「正規のエンジニア『予備軍』だが、他のメンバーには持っていないスキルを持っているエンジニア(インフラ技術に特化した人材)」を選定しています。そして、そのチームは自動化・省力化するために積極的にプログラムを書く。SREが「System Administrator」ではない最大の特徴は、システムの拡大に伴い、保守運用工数が正比例して増大することのないように、自分たちでプログラムを書いて積極的に自動化・省力化を行います。が今更、そんなことを言うつもりはない。SREで大事なところはそこではない。大切なのは役割や役職としてのSREではなくSREのプラクティスであると思う。「SREの探求」でもそれは１言及されているので異論がある方は読んでから議論しましょう。www.oreilly.co.jpインフラエンジニアからSREになるということSRE　≠　インフラエンジニアという認識は皆さんにもあるとは思います。そもそも、開発力が皆無でコードの読み書きが選択肢になければ問題が起きた時に挙動を把握し続け、ログやメトリクスでとことん確かめていきます。何が起きてるか事実ベースで完全に外からの推論だけで把握するというのは難しい、もしくは不完全です。コードを読めなければ問題を解決するのはいつも本当の偶然か、計画的な偶然か、もしくはそれらを超越した熟練者の直感によるものが多いです。もちろん、直感は素晴らしいです。なぜなら、熟練者の直感は問題が起きた時に直感で全ての問題を見つけて解決していくからです。それらは、高度で本人にさえ言語化不能で本当に超能力者みたいです。私はそのような能力をまだ持ち合わせていないです。が、問題は起きていて解決は迫られてます。直感を身につけるような時間はもう、残されていません。明日の朝にはこの問題を解決して納品しなければリリースに間に合わない。解決や期間的な目処は立てなければなりません。もし、解決方法もどれだけで解決するかも分からないとなればプロジェクトに関わるメンバーは不安になると思います。自社開発のソフトウェアで、自社のエンジニアが書いたコードあればそのエンジニアか所属する組織への依頼をすれば良いでしょう。ですがSREは自社のソフトウェアだけではなくOSSと向き合っていることも多く不具合を気軽に相談できない場合も多いです。私に起こった不具合が設定値によるものなのか？ 実装によるものなのか？ を切り分けができなければOSSのIssue も出すことができません。ちなみに、有償サポートでは、その限りではないと思います(有償サポートがあるなら頼れ)。ソフトウェアを読めることで、問題が起きた時にコードを読み尽くして、負荷試験や本番で挙動を把握し続け、ログやメトリクスでとことん確かめていけばシステムに対する解像度が上がります。プログラムをちゃんと読めて原因の特定、修正がコードレベルでできるようになるという選択肢は確実性を高めます。そのため、ソフトウェアの開発経験がないエンジニアのためにOJTに開発経験を追加してそういった素地を最初に作ってもらいたいと思っています。システムへの解像度をあげてエンジニアとしてのレベルを上げる問題が起こったらログやメトリクスから情報を収集して推論して対応していくことも当たり前のように重要である。しかし、運用をしていたらそれでは解決しない問題にぶつかることもある。そのため、以下のような習慣を是非、身につけてほしいと思っています。各項目についてはこのブログでは深く記載しませんがぜひ、深堀りしてみてください。1.コードがあったら読むコードが読めなければ障害やアラートの原因が最終的に神の怒りになる。OSSの動きが不思議だったら、外から動作を推測するよりコードを読んだ方が確実案件でコードが公開されているなら、それもコードを読んだほうが確実天才ではない限り読んだことの無いコードは書けない。2.Strace でシステムコールを追うLinux にはあるプロセスが呼び出している system call を確認できるコマンドがある使えるといろいろと重宝するので使えるようになりましょう例)パーミッションエラーが発生したがどこが悪いのかログに出てきてない！こんなときにはStraceだ！！！最終的にはLinux プログラミングインターフェースと旅行に行きましょう！それでもダメなら起動せよ！デバッガー(gdbとdelve)！CPUやメモリのボトルネックを調査するためにプロファイリングツール(pprof など) を利用する3. ネットワークトラブル時はTCPパケットを追うパケットは友達！パケットは嘘を付きません！RSTが返されているとか、応答自体がきていないなどの切り分けが必要ログやメトリクスだけではどうしても追いきれない場合がある4.技術ドキュメントを書く検討結果はもちろん、検討過程や思考を記録に残すことが大事色々なやり方がある場合には「〜をする5つの方法」のような書き方構造品質を保つ(構成が適切か、単語ミスはないか、口調はそろっているか、文法は正しいか、textlintをかけているか、構成テンプレートに沿っているか)機能品質を保つ(サービスやビジネスの継続に価値を発揮できているか？)5. OSSのバグがあったら報告するOSS のドキュメントでも良いのでフリーライドしすぎない開発力が必要になるいつか、一緒に働いてくれるかもしれない誰かが見てるかもしれないさいごにというような社内ポエムを爆誕させていたらそこそこ反応が良かったので再編集してブログにしておきました。もちろん、開発力も大事ですがそれらを通して原理原則を理解して自走しながら課題解決し続ける人の方がもっと価値のあることだとは思っております。皆さんSREへの修行もしくは鍛錬がてら入社を待ってます！3-shake.com去年から同じようなこと言っていてますね、、。syu-m-5151.hatenablog.com","link":"https://syu-m-5151.hatenablog.com/entry/2022/06/23/153827","isoDate":"2022-06-23T06:38:27.000Z","dateMiliSeconds":1655966307000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"朝活として最高の書籍である『システム運用アンチパターン』を読んだので読書感想文","contentSnippet":"はじめにSREという信頼性の観点からのプラクティスや運用技術を実施出来るためのプロダクトの開発をしている身からすると『システム運用アンチパターン』はまさに様々な課題がわかりやすく言語化されており素晴らしい書籍で、熟練の運用エンジニアとお話ができるような経験ができました。このエントリーは『システム運用アンチパターン』を読んでみた中での感想文となります。www.oreilly.co.jp『システム運用アンチパターン』目次1章　DevOpsを構成するもの1.1　DevOpsとは？1.2　DevOpsの柱となるCAMS1.3　また別のDevOps本？1.4　本章のまとめ2章　パターナリスト症候群2.1　安全装置ではなく障壁を作ってしまう2.2　ゲートキーパーの導入2.3　ゲートキーパーの分析2.4　自動化によるパターナリスト症候群の解消2.5　承認の目的を把握する2.6　自動化のためのコードの構成2.7　継続的な改善に向けて2.8　本章のまとめ3章　盲目状態での運用3.1　苦労話3.2　開発と運用の役割を変える3.3　プロダクトの理解3.4　運用の可視化3.5　ログを価値のあるものにする3.6　本章のまとめ4章　情報ではなくデータ4.1　データではなく利用者から始める4.2　ウィジェット：ダッシュボードの構成要素4.3　ウィジェットに文脈を与える4.4　ダッシュボードの構成4.5　ダッシュボードの命名4.6　本章のまとめ5章　最後の味付けとしての品質5.1　テストピラミッド5.2　テストの構造5.3　テストスイートの信頼性5.4　継続的デプロイと継続的デリバリ5.5　機能フラグ5.6　パイプラインの実行5.7　テストインフラの管理5.8　DevSecOps5.9　本章のまとめ6章　アラート疲れ6.1　苦労話6.2　オンコールローテーションの目的6.3　オンコールローテーションの設定6.4　アラート基準6.5　オンコールローテーションの配置6.6　オンコールへの補償6.7　オンコールの幸福度を追跡する6.8　オンコール担当中のタスク6.9　本章のまとめ7章　空の道具箱7.1　社内ツールと自動化が重要な理由7.2　なぜ組織はもっと自動化しないのか7.3　自動化に関する文化の問題を解決する7.4　自動化を優先する7.5　自動化の目標を決める7.6　スキルセットのギャップを埋める7.7　自動化のアプローチ7.8　本章のまとめ8章　業務時間外のデプロイ8.1　苦労話8.2　デプロイのレイヤ8.3　デプロイを日常的に行う8.4　頻繁に行うことで恐怖心を減らす8.5　リスクを減らして恐怖心を減らす8.6　デプロイプロセスの各レイヤでの失敗への対応8.7　デプロイアーティファクトの作成8.8　デプロイパイプラインの自動化8.9　本章のまとめ9章　せっかくのインシデントを無駄にする9.1　良いポストモーテムの構成要素9.2　インシデントの発生9.3　ポストモーテムの実施9.4　本章のまとめ10章　情報のため込み：ブレントだけが知っている10.1　どのように情報がため込まれているかを理解する10.2　意図せずに情報をため込んでいる人を認識する10.3　コミュニケーションを効果的に構築する10.4　知識を発見可能にする10.5　チャットツールの有効活用10.6　本章のまとめ11章　命じられた文化11.1　文化とは何か？11.2　文化はどのように行動に影響を与えるか？11.3　文化を変えるには？11.4　文化に合った人材11.5　本章のまとめ12章　多すぎる尺度12.1　目標の階層    12.1.1　組織の目標    12.1.2　部門の目標    12.1.3　チームの目標    12.1.4　目標の確認12.2　どの仕事に取り掛かるかに意識的になる    12.2.1　優先度、緊急度、重要度    12.2.2　アイゼンハワーの意思決定マトリックス    12.2.3　コミットメントにノーと言う方法12.3　チームの仕事を整理する    12.3.1　作業を細分化する    12.3.2　イテレーションの作成12.4　予定外の作業    12.4.1　予定外の作業のコントロール    12.4.2　予定外の作業への対応12.5　本章のまとめ本書のまとめ訳者あとがき索引特徴と感想ハードスキルというよりソフトスキルを得るための書籍である本書のタイトルや目次を見ると、本書はDevOpsの概念やアンチパターンの紹介の書籍かと思われるが CAMS(文化、自動化、メトリクス、共有)についてのソフトスキル(ツールや道具ではなく)を組織や個人で実践するためのHowTo本だという印象を受けました。運用(に関わるソフトウェア)エンジニア版の7つの習慣(語弊あり)。それぞれの章は奥が深く、章で取り上げているものはソフトスキルに絞って知識をバランス良く記載していると感じました。DevOpsとSREの違い今からSREのキャリアを目指す方には混乱させてしまうかもしれないのでざっくり、DevOpsとSREの違いについて少し解説していきます。「class SRE implements DevOps」という考えが良い回答かなと思います。「class SRE implements DevOps」は、「SREはDevOpsというinterfaceの実装である」という意味を表します。「DevOps = 思想」という定義に対し、それを具体化し実装したものがSREであるという考えです。DevOpsにも以下の5点のような考え方がありSREにも似たような考え方がありますよね？組織のサイロの削減（風通しのよい組織の実現）エラー発生を前提とする（100%を目指さない）段階的に変更を行う（一気にすべてを変更しない）ツールと自動化を活用する（サービス成長と正比例で運用工数を増やさない）全てを計測する（モニタリングに基づく数値設定が重要）ただ、上記はあくまで思想、概念でしかなく、具体的な方法論ではありません。これを具体的にどのように行うかを「トイルの削減・自動化」「SLI/SLOの設定による目標定量化」といった形で、誰でも用いられるように体系化させたものがSREといえます。本書はDevOpsのソフトスキル面を主に扱ってるそのため、ソフトウェアの運用に関わる人であればどのレベルの人が読んでも良いと思います。また、本書はSREを目指す方が読んでも学びになる書籍だと思います。まとめ「システム運用アンチパターン」はシステム運用に関わったことがある人があの時、本書を読んでいればなにかが変わったかもしれないと思えるほど学びのある。なぜ、それがイケてないか優しく説明してくれる素晴らしい熟練の先輩との対話のような一冊でした。特に本書の『DevOps文化は必ずしもA地点からB地点へ進むようなものではないことを覚えておいてください。』という言葉はカンファレンスや技術ブログ記事で紹介されているツールやワークフローにすぐに飛びつきたくなる私のような無知で軽率な若者にはとても響きました。それよりもソフトスキルを組織に根付かせることの重要性を様々な視点から本書は教えてくれました。みなさんも『システム運用アンチパターン』をぜひ手に取ってみてください。おまけGoでの開発やSRE/DevOps について雑談 をしたい方を募集してますー！雑談する予定しかしないのですが仕事の話でもキャリアの話でも学生の方でも社会人の方でも気軽にお待ちしております。meety.net参考O'Reilly Japan - システム運用アンチパターンGoogle - Site Reliability Engineering","link":"https://syu-m-5151.hatenablog.com/entry/2022/05/27/070239","isoDate":"2022-05-26T22:02:39.000Z","dateMiliSeconds":1653602559000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"PrometheusのSLO generator であるSloth について雑多な作業ログ入門失敗編","contentSnippet":"はじめにこのブログではPrometheus のSLO generator であるSloth に関して取り上げたいと思っている。正直、業務後の深夜に書いているのでGrafana でDashboards を生成したら感動的なラストシーンということにしてほしい。今回は取り上げないが最近、バージョン1.0.0 になったOpenSLOの 周辺知識も気になっている。OpenSLOについて | フューチャー技術ブログ などはOpenSLOに関して2022 年5月現在で日本語で書かれている文章だと最高に良いと思います。 ナマケモノのイラスト | かわいいフリー素材集 いらすとや より引用SLIを計測しSLOを設定するいきなり、Slothの話をするのも流石に不親切なのでSRE的な話を少しだけします。ITサービスの運用に置いて、信頼性100%と、信頼性99.99%では大きな違いがあります。信頼性100%を実現するためには、99.99%とは異なり膨大な工数を投入する必要があるが、ほとんどのユーザーにとっては「99.999%」が「100%」になったからといって、大きなメリットがないことのほうが多いのである。つまり、100%を目指すことは効率的ではない場面が多いため、各サービスごとに適切な可用性を設定する必要だ。はじめに、SLIですが、これは「Service Level indicator」の略で、提供されているサービスのレベルの性質を定義した計測量である。一般的には以下をSLIとして用いる。リクエストのレイテンシ（リクエストに対するレスポンスを返すまでにかかった時間）エラー率（受信したリクエストを正常に処理できなかった比率）システムスループット（単位時間あたりに処理できるリクエスト数）可用性（サービスが利用できる時間の比率）次に、SLOですが、これは「Service Level Objective」の略で、SLIで計測されるサービスレベルの目標値、または目標値の範囲を指します。例えば、SLOを「年99.99%」と設定すると、「1年のうち52分は稼働しなくてもよい」ということになる。例えば、「1年の間にサービスが30分停止する障害」が生じたとしても、SLOの範囲であればそれは想定の範囲であり、問題ではなくなる。同様の用語で、SLA というのがある。これは「Service Level Agreement」の略で、こちらはITサービスの契約において「この稼働率を下回る場合、金銭的な保証を行う」ことを示す値です。SLIは測定値、SLOは補償を伴わない目標値である点で意味合いが異なる。Sloth の特徴で、Sloth はPrometheusベースのSLOを作成するために、複雑な仕様やプロセスを把握して使用する必要がないように。迅速、簡単、かつ信頼性の高いPrometheus SLO generator(生成)してくれる。生成された記録とアラートのルールに基づき、信頼性の高い均一なSLOの実装を実現します。Kubernetes でCRDなどを用いてサポートしており、OpenSLOも限定的にサポートしています。Sloth が生成するPrometheusのルールは3つのカテゴリーに分類されます。1つ目がSLIです。SlothにおけるSLOはルールはベースとなるもので、ユーザーから提供されたクエリを使用して、エラーサービスレベル（可用性）が何であるかを示すために使用される値を取得するものです。異なる時間帯に対して複数のルールを作成し、これらの異なる結果がアラートに使用されます。2つ目がMetadataです。これらは、残りのエラーバジェットやSLO目標パーセントのような有益なメトリックとして使用されます。これらは、Grafanaダッシュボードなど、SLOの可視化に非常に便利です。3つ目がAlerts でSLIルールに基づくマルチウィンドウ・マルチバーンアラートです。Sloth はサービスレベル仕様書を受け取り、仕様書の各SLOについて、上記のカテゴリーで3つのルールグループを作成します。MetricsSlothが生成したルールは、SLO間で同じメトリック名を共有します。しかし、ラベルは異なるサービス、SLOを識別するためのキーとなる。このようにして、異なるチームやサービス間で、すべてのSLOを記述する統一された方法を得ることが出来ます。Slothが作成し、利用可能なすべてのメトリック名を取得するには、次のクエリを使用します。count({sloth_id!=\"\"}) by (__name__)AlertSlothの SLOアラートは、マルチウィンドウ・マルチバーン方式を採用し、Critical/page とWarning/ticketの2種類のアラートを生成します。また、時間帯によって4種類のAlertを使用します(が割愛)。また、Sloth は自らAlertを発するのではなく、Slothが生成したAlertルールを使ってPrometheusがAlertを発する。Prometheusに接続されたalertmanagerを介してSlack、Pagerdutyなど に通知をトリガーします。sloth.devやっていくGetting started - Sloth を参考にKubernetes上に構築をやっていくPromethus 周りのインストール# Get Helm Repository Infohelm repo add prometheus-community https://prometheus-community.github.io/helm-chartshelm repo update# Install Helm Chart :https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack#configurationhelm install [RELEASE_NAME] prometheus-community/kube-prometheus-stackSloth のインストールSloth のhelm はこちらにあるので参照してください。github.comhelm repo add sloth https://slok.github.io/slothhelm repo updatehelm install [RELEASE_NAME] sloth/sloth もしくはCRDをデプロイしていきましょう# Sloth CRD is required$ kubectl apply -f ./pkg/kubernetes/gen/crd/sloth.slok.dev_prometheusservicelevels.yaml# Prometheus Operator Rules CRD is required$ kubectl apply -f ./test/integration/crd/prometheus-operator-crd.yamlhelpコマンドをインストールしたらhelp を見る良い習慣です。sloth helpusage: sloth [<flags>] <command> [<args> ...]Easy SLO generator.Flags:  --help            Show context-sensitive help (also try --help-long and --help-man).  --debug           Enable debug mode.  --no-log          Disable logger.  --no-color        Disable logger color.  --logger=default  Selects the logger type.Commands:  help [<command>...]    Show help.  generate [<flags>]    Generates Prometheus SLOs.  kubernetes-controller [<flags>]    Runs Sloth in Kubernetes controller/operator mode.  validate --input=INPUT [<flags>]    Validates the SLO manifests and generation of Prometheus SLOs.  version    Shows version.generatedget-started.ymlと同じ例ですが、Kubernetes で実行するのでCRDを使用した例です。Kubernetesのprometheus-operator PrometheusRules CRDにPrometheusのルールを生成します。sloth.devk8s-getting-started.yml をsloth generate させます。ちなみにnamespace を namespace: default にして実行いたします。$ sloth generate -i k8s-getting-started.yml INFO[0000] SLI plugins loaded                            plugins=0 svc=storage.FileSLIPlugin version=1912e6a window=30d                                                            INFO[0000] SLO period windows loaded                     svc=alert.WindowsRepo version=1912e6a window=30d windows=2                                                                INFO[0000] Generating from Kubernetes Prometheus spec    version=1912e6a window=30d                                                                                                INFO[0000] Multiwindow-multiburn alerts generated        out=- slo=myservice-requests-availability svc=generate.prometheus.Service version=1912e6a window=30d                      INFO[0000] SLI recording rules generated                 out=- rules=8 slo=myservice-requests-availability svc=generate.prometheus.Service version=1912e6a window=30d              INFO[0000] Metadata recording rules generated            out=- rules=7 slo=myservice-requests-availability svc=generate.prometheus.Service version=1912e6a window=30d              INFO[0000] SLO alert rules generated                     out=- rules=2 slo=myservice-requests-availability svc=generate.prometheus.Service version=1912e6a window=30d~~~k8s-getting-started.yml を元にさまざまなファイルが生成されています！眠いので解説はしません# This example shows the same example as getting-started.yml but using Sloth Kubernetes CRD.# It will generate the Prometheus rules in a Kubernetes prometheus-operator PrometheusRules CRD.## `sloth generate -i ./examples/k8s-getting-started.yml`#apiVersion: sloth.slok.dev/v1kind: PrometheusServiceLevelmetadata:  name: sloth-slo-my-service  namespace: monitoringspec:  service: \"myservice\"  labels:    owner: \"myteam\"    repo: \"myorg/myservice\"    tier: \"2\"  slos:    - name: \"requests-availability\"      objective: 99.9      description: \"Common SLO based on availability for HTTP request responses.\"      sli:        events:          errorQuery: sum(rate(http_request_duration_seconds_count{job=\"myservice\",code=~\"(5..|429)\"}[{{.window}}]))          totalQuery: sum(rate(http_request_duration_seconds_count{job=\"myservice\"}[{{.window}}]))      alerting:        name: MyServiceHighErrorRate        labels:          category: \"availability\"        annotations:          summary: \"High error rate on 'myservice' requests responses\"        pageAlert:          labels:            severity: pageteam            routing_key: myteam        ticketAlert:          labels:            severity: \"slack\"            slack_channel: \"#alerts-myteam\"validate構文チェックも可能です$ sloth validate --input=k8s-getting-started.ymlINFO[0000] SLI plugins loaded                            plugins=0 svc=storage.FileSLIPlugin version=1912e6a window=30dINFO[0000] SLO period windows loaded                     svc=alert.WindowsRepo version=1912e6a window=30d windows=2INFO[0000] Validation succeeded                          slo-specs=1 version=1912e6a window=30ddeploygenerate したものをapply していきます$ sloth generate -i k8s-getting-started.yml | kubectl apply -f -Grafana へのログインkube-prometheus のGrafanaは初期ID/PASS のadmin:admin ではないのでパスワードを確認する(ArgoCD でも似たように初期パスワードを取得できる)# user:password -> admin:prom-operator$ kubectl get secret sloth-grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echoprom-operatorGrafana へのDashBoard の追加SLO / Detail dashboard for Grafana | Grafana Labs入門失敗Dashboards を生成したら感動的なラストシーン！？このダッシュボードには、各SLOの詳細が表示されますがこれには、http_request_duration_seconds_count にデータが入ってないわ。。。          errorQuery: sum(rate(http_request_duration_seconds_count{job=\"myservice\",code=~\"(5..|429)\"}[{{.window}}]))          totalQuery: sum(rate(http_request_duration_seconds_count{job=\"myservice\"}[{{.window}}]))Dashboards - Sloth にしたいんですけど,,, もう眠いので一旦、終わって公開します。Sloth はとりあえず動かすためのチュートリアルが絶妙に弱くてPrometheusをある程度理解してないとうまく動かせないなーって思いました(小物として)。参考Sloth - SlothSLO / Detail dashboard for Grafana | Grafana LabsPrometheus - Monitoring system & time series database","link":"https://syu-m-5151.hatenablog.com/entry/2022/05/25/165633","isoDate":"2022-05-25T07:56:33.000Z","dateMiliSeconds":1653465393000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"ごめん、同窓会にはいけません。今、株式会社スリーシェイクにてソフトウェアエンジニアとして働いております。","contentSnippet":"はじめにこのブログは入社エントリーになります。帰属意識が低い訳でもないのに、どの同窓会に誘われてない私の26歳が終わってしまった。2022年の5月10日には27歳になったりもします。The 27 Clubじゃんと思ったけど何も残してない。26歳は新卒で入社した会社からSREの会社に転職をしました。ちなみに、去年の誕生日には最終出社をキメました。孤独独身男性なもんで26歳の誕生日を1人で焼肉やってる！！！ pic.twitter.com/zuDgnUsibY— nwiizo (@nwiizo) 2021年5月10日  転職後たくさん手を動かせているので個人的には転職に満足しております。また、技術的な事以外でいうと、オンラインでの登壇やアウトプットが増える施策をいくつか社内外で実施できた。これらを達成する為に人を巻き込んで人に頼ることができてるようになったことに自身の成長を感じております。ちなみに新卒で入った会社でしたが送別会などは(コロナを理由に)ありませんでした。やっていき去年の2021年6月1日から株式会社スリーシェイクにてソフトウェアエンジニアとして働いております。2021年06月01日に入社して12ヶ月目に突入いたしましたnwiizoです。\"インフラをシンプルにしてイノベーションが起こりやすい世界を作る\" でお馴染みの株式会社スリーシェイクにてソフトウェアエンジニアとして開発をさせていただいております。一日でも早く入社エントリーを書こうと思っていたら、11ヶ月も経過しておりました。以前より入社エントリーを入社前や直後に書いて理想ばかり語ってるのを見て青く眩しいなって思っていたのでちょうどいいかもしれません。今は怠惰な自分の性格を正当化しました。この記事では入社した理由と株式会社スリーシェイクの魅力について語っていきたいと思います。このブログは絶対的に入社エントリーになります。なぜ、前職を辞めたのか？2017年の新卒で入社したGMOインターネット株式会社を4年2ヶ月で退職いたしました。前職ではホスティングサービスの開発と運用、お名前.comやいくつかの商材サイトが載っている社内コンテナ基盤の開発と運用、エバンジェリストとしての業務を行なっていました。また、新卒エンジニアの技術力向上・適性判断を目的とした研修プログラムでコンテナ技術の講師を任せていただけたりと色々幅広くやってました。退職までのざっくりとした経緯や理由については元々、前職に入社したのは『俺が考える最強のソフトウェア基盤をサービスとして世の中に提供したいよー、いろんなパブリッククラウドに負けたくないよ〜』という感情からでした。入社後、わりと大きな大義を持って働いていたのでたくさんのチャンスをいただきました。が、様々な面での実力不足で実際のサービスの提供に至れませんでした。が、GMOインターネットという会社は「手を上げる文化」を大切にしており、特に新卒エンジニアにはチャンスをくれるいい会社だと思うのでこれを見てる学生の皆さんはオススメです。ので、リンクを貼っておきます。退職時のエントリーsyu-m-5151.hatenablog.com転職についてそんなこんなで、2017年から新卒で入社していろいろやらせてもらってました。が、これからも引き続きサービスの開発を行うために尽力する為に、会社に残る選択肢はありましたが、このまま、残ってもやりたいことをやりきるだけの力(コーディング力云々は除く)を身につけることはできねーなというようなある種の閉塞感みたいなものに心が囚われるようになり(完全なる言い訳)、このまま続けてもだらだらになってしまう気がして良くない。時間は有限だぞ!!！ と、転職することを決意しました。また、第二の理由に家庭の事情があります。前職では「地方でフルリモートワークの業務」というのは原則認められてませんでした。転職のタイミングで、実家の事情で「地方でフルリモートワークの業務」という希望を伝えた上でお声掛けいただいた会社さんと話をさせてもらい、最終的にスリーシェイクに参加することにしました。で元々は「地方でフルリモートワークの業務」を考えていたのですが、家族の助けもあり諸々が解決したおかげで東京に残る事が出来ました。自分自身がオフラインのカンファレンスに刺激をもらって向上心をキープしている部分があり、東京でのイベントに出かけやすい東京に残れて、本当によかったです。スリーシェイクのポイント手段の為なら目的を選ばないタイプのソフトウェア技術者としてSREやKubernetesを中心としたCloud Nativeな技術領域に関わっていきたいと漠然と考えていた。その中でスリーシェイクは技術領域はもちろん、会社としてのビジョンや掲げていることが気に入り、この会社で働いてみたいというのが面談、面接を通して更に強くなっていったからです。あとは、ひとつの会社に所属するだけでいろんな組織やチームのSREとして働けるなんてお得じゃんと思ってしまいました()。何よりも「インフラをシンプルにしてイノベーションが起こりやすい世界を作る」や「社会に蔓延る労苦〈Toil〉をなくすプラットフォーマーになる」をどれだけ躓いても全力でやりきっていける組織だと自分が思ったからです。NARUTOが以前「オレが知りてーのは楽な道のりじゃねェ 険しい道の歩き方だ」ということを言ってましたが吉田 拓真 / スリーシェイク (@taqqma_ikachan) | Twitter は本当にこんな感じのことを毎月の全社会でよく言ってます。スリーシェイクでやっていくことインフラエンジニアっぽいSRE として入社しました。が現在は、いわゆるバックエンドエンジニアとしてSREのような信頼性の観点からのプラクティスや運用技術を実施出来るためのプロダクトの開発をしております。毎週、SRE Weekly を熟読して、こういうプロダクトを作りたいと常々思うようになっていたので本当に夢のようなお話です。また、アウトプットしないのは知的な便秘ということで引き続きアウトプットもやっていきます。Meetyもやっているのでみんなお話ししましょう。meety.netSREやインフラエンジニアだけではなく様々な職種の募集をしているので皆様！！！！3-shake.com最後に下記のリンクに皆さんへの日々の感謝を正拳突きで表現させていただいております。入社エントリーでもあるのですが明日は誕生日です。いつもありがとうございます！www.amazon.co.jp","link":"https://syu-m-5151.hatenablog.com/entry/2022/05/09/171305","isoDate":"2022-05-09T08:13:05.000Z","dateMiliSeconds":1652083985000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"インフラエンジニアが学ぶと良さそうなgRPCサーバーについて","contentSnippet":"3-shake にはSreake共有会 という毎週、火曜日と木曜日に担当者が現場で得た知見などを発表する社内勉強会が開催されています。こちらのブログはそれらを変更修正しております。syu-m-5151.hatenablog.com元々しようとしていたの話Go 1.18 の最新情報←Generics の深い話とかはもう既出すぎて気になる人は読んでるGo でのTDD(が実は20周年なので)←書いてる途中で自分が言うべきことなんてないことに気付く今後、案件で増えるであろう gRPC についてインフラエンジニアが知っておいても良いと思ったという話 ← 今ここTL;DRprotobuf (Protocol Buffers) はデータフォーマットで、JSONの役割を置き換えるものです。一方 gRPC は通信プロトコルで、HTTPの役割を置き換えるものです。gRPC をライブラリやツール、トレンドなどを通してgRPCを知るgrpc.ioRPCとはgRPC がこの世の中に急に爆誕したわけではない。そもそも、サービス間での情報のやり取りをどのように行うかというのは古くからある課題の1つです。その中で利用されているのがRPCがあります。RPCとは、Remote Procedure Callの略で遠隔手続き呼出しと訳されます。すなわち、別の場所にあるプログラムを呼び出そうというのを目的としています。違うアプリケーションロジックをあたかも自分のアプリケーションの処理と同じように扱えることができるというのも特徴です。クライアントはサーバーに対し実行する処理を指定するパラメータや引数として与えるデータを送信し、それに対しサーバーはパラメータに応じた処理を実行してその結果をクライアントに返す、というのがRPCの基本的な流れになります。ちなみに、gRPC以外にもJSON-RPC、XML-RPCなどがあります。gRPCの歴史gRPCは、Googleが開発したRPC技術がベースとなっている。Googleでは多数のコンポーネントを組み合わせてサービスを実現しています。いわゆる、マイクロサービスアーキテクチャでシステムを構築していることで知られるが、これらのサービス間で通信を行うためにgRPCの前身Stubbyと呼ばれる技術が開発されました。ただ、StubbyはGoogleのインフラ以外での利用は想定しておらず、独自の仕様が多く、Stubby で使用されていた技術とコンセプトが近いHTTP/2 などの技術が登場したことから、GoogleはStubbyにこれらの技術を取り入れてオープン化することを決め、それがgRPCです。なお、現在ではgRPCはオープンソースで公開されており、現在はLinux Foundation傘下のCloud Native Computing Foundation（CNCF）によって開発が進められています。grpc.iogRPC についてgRPCではほかのRPCと同様、クライアントがサーバーに対しリクエストを送信し、サーバーはそれに応じた処理を実行してその結果を返すという、クライアント−サーバーモデルを採用している。gRPCでは以下のような特徴があります。HTTP/2 による高速な通信バイナリにシリアライズされて送られてくる小さな容量で転送できる一つのコネクションで複数のres/reqが可能(柔軟なストリーミング形式)ミドルウェアの設定でハマった時にはHTTP/2 の問題なのかgRPCの問題なのか切り分ける必要があると思います。Protocol buffersgRPCではProtocol Buffersのサービス定義ファイルからサーバーおよびクライアント向けのコードを自動的に生成するツールが提供されており、これを利用することで簡単にサーバーおよびクライアントを実装できるようになっている。そのため、クライアントとサーバーが異なる言語で実装されていても、問題なく通信を行うことができるようになっている。クライアント・サーバー間の通信に使用するプロトコル（トランスポート）や、やり取りするデータの表現およびシリアライズ方法については置き換えが可能な設計になっているが、デフォルトではトランスポートにHTTP/2が、データのシリアライズにはProtocol Buffersという技術を使用するようになっており、これをそのまま使用するのが一般的です。Protocol BuffersはGoogleが開発したデータフォーマットで、バイナリデータを含むデータでも効率的に扱えるのが特徴です。このProtocol Buffersについても、さまざまなプラットフォーム・プログラミング言語から利用できるライブラリが提供されている。柔軟なストリーミング形式単方向/双方向ストリーミングRPCに対応している。ちなみに私はこの仕様をきちんと把握してなくて2度辛い思いをしているので記憶の片隅においておいてください。Unary RPC1つのリクエストに対して一つのレスポンスを返す一般的な通信です。誤解を恐れぬ言い方をするとREST API のような挙動。Server streaming RPCクライアントから送られてきた一つのリクエストに対して、複数回に分けてレスポンスを返す通信方式です。最後のレスポンスを返した後も任意にサーバーの情報を変更に応じてクライアントにその情報を送ることができます。Client streaming RPCクライアントからリクエストを分割して送ってサーバーはすべてのリクエストを受け取ってからレスポンスを返します。大きなデータをPOSTしたいときに便利です。Bidirectional streaming RPCクライアントからリクエストが送られてきたときにサーバーとクライアントは一つのコネクションを確立しお互いに任意のタイミングでリクエストとレスポンスを送りあうことが可能になります。他のプロトコルとの違いと連携Web サービスやマイクロサービスで使われるプロトコルの代表格は HTTP/HTTPS と、それを利用した REST API です。 HTTP は非常に柔軟ですが、渡すデータのスキーマが標準化されていないため、異なる言語間の RPC を実装するのは面倒です。 OpenAPI という REST API 用の IDL もありますが、Protocol Buffers と比較すると記述量が多いです。また、JSONとprotobufの重要な違いとして、protobufはフォーマットがスキーマに依存するという点があります。JSONはスキーマがなくても完全なシリアライズ・デシリアライズが可能ですが、protobufのデータをシリアライズ・デシリアライズするにはスキーマ情報が必要です。gRPCは技術的には必ずしもスキーマ依存ではありませんが、実装上はスキーマなしで実装するのは困難です。この技術的制約によりスキーマファースト開発が強制されるのが protobuf + gRPC の強みのひとつです。よく言われるのが、GraphQL です。GraphQL は Facebook が開発したプロトコルで、HTTP 上で処理されますが REST API とは異なり GET/POST などのメソッドやステータスコードに意味を持たせていません。 特徴はスキーマはデータ構造を定義するもので、標準化されたクエリにより任意のデータを取得可能な仕組みになっていることです。gRPC がどのようなものか？gRPC Motivation and Design Principles によればgRPCの基本的なコンセプトとして次のものが挙げられている。サービスはオブジェクトではなく、メッセージはリファレンス（参照）ではない適切な適用範囲とシンプルさフリーかつオープン相互運用性があり、一般的なインターネットインフラ内で利用できる汎用性がありながら、専用のものと比べてパフォーマンス面で一般に劣らないアプリケーションレイヤーと分離された構造ペイロードを問わないストリーミングでの情報伝達に対応同期・非同期の両方に対応通信の中断やタイムアウトをサポート確立された通信を処理しつつ新規接続を止めるようなシャットダウンのサポートデータ流量のコントロール機能デフォルト実装に対して後からさまざまな機能を追加可能APIによる機能拡張が可能メタデータの交換をサポート標準化されたステータスコードこのようなものを頭に叩き込んでいると様々な場面でgRPCの設計がどのような思考でそのようになされているか分かる。gRPC Ecosystemgithub.comgRPCを補完するgRPCエコシステムとして各種サービスが紹介されている。ヘルスチェックやPrometheus での設定などがこちらに紹介されているgRPC-WebgRPC-WebによってgRPC通信をWebでも使うことができる。HTTPサーバーが仲介者として機能することなく、WebアプリがgRPCバックエンドサービスと直接通信できるようになるものです。またクライアントもバックエンドもgRPCでの実装なので完全なエンドツーエンドのgRPCサービスアーキテクチャを作成できることが利点です。protoファイルに記述したらあとは、お互い実装ができるので開発も進められやすいです。github.comgRPC-Gatewayprotoファイルに書かれたサービスの定義を理解し、REST APIに変換できます 。gRPC-GatewayだけでRESTfulなAPIを受け取れます。また、protoファイルからswagger.jsonを自動出力してくれる機能も備わっており、ドキュメント生成に関しても十分です。grpc-ecosystem.github.ioenvoygRPC-GatewayとenvoyはどちらもJsonをgRPCに変換してくれる機能を持ち合わせています。JSONを変換してくれるだけよくGolangでの実装だったら、gRPC-Gatewayでいいのかなと思いますがそれ以外にはEnvoy にはさまざまな機能があるので一気に全部やってしまいたい方にはEnvoyの利用を考えても良いのかな？と思います。www.envoyproxy.iogRPC をライブラリやツールについてインフラエンジニアがgRPC に関わる時は開発というより運用や保守に関してだろう。なので、今回、紹介するツールもそれらに沿って紹介したい。ツールの使い方を調べれば自ずとgRPCの輪郭が見えてくるかと思います。Awesome gRPC はgRPC に 関するキュレーションを行うリポジトリ。大体のツールはここを確認すれば良い。https://github.com/grpc-ecosystem/awesome-grpcgrpc_cligrpc/command_line_tool.md at master · grpc/grpcgRPC の公式リポジトリに同梱されている grpc_cli は公式の gRPC クライアントツールといえますが、最低限の機能しか備えていません。例えば他の gRPC クライアントツールではほぼ実装されているメタデータの送信ができない、JSON 形式でのリクエスト内容の記述を受け付けられないといった問題があります。また、インストールするためにはソースコードからビルドする必要があり煩雑なのであまり、使われていません。gRPCurlhttps://github.com/fullstorydev/grpcurl最も使われている gRPC クライアントツールです。現在も活発にメンテナンスされています。機能面でもたいていのユースケースは網羅されており、機能の不足で困るようなことはほとんどないでしょう。prototoolhttps://github.com/uber/prototoolPrototoolは Uber Technologies によって開発された Protocol Buffers のユーティリティツールです。Prototool には gRPC のエンドポイントを呼び出せるサブコマンドが付属しています。ただし、このサブコマンドは fullstorydev/grpcurl に大きく依存しており、実質 gRPCurl のサブセットとなっています。現在は Protocol Buffers のユーティリティツールとして Buf を推奨するしています。Bufhttps://github.com/bufbuild/bufProtocol Buffers のユーティリティツール 戦争に勝ち抜いたと言っても良い buf は自動ファイル検出、正確なlintとbreaking checkersの構成を選択することができたり、エラー出力はどのエディターでも簡単に解析可能(vs Code はさまざまなツールが動くが、vim はこれぐらいしか、プラグインがうまく動かない)、コンパイルの高速化、protocのプロトコルプラグインとして使用する。gRPCUIhttps://github.com/fullstorydev/grpcuigrpcuiは、ブラウザ経由でgRPCサーバと対話するためのコマンドラインツールです。Postman のようなものですが、REST ではなく gRPC API のためのものです。evansgRPC クライアントツールです。REPL モードで手軽に手動テストを行えますのでデバッグの時にあるとめちゃくちゃ便利です。https://github.com/ktr0731/evansJSON-to-ProtoJSONを即座にProtobufに変換してくれるツールになります。JSON-to-Proto次回予告:gRPC を使ったアプリケーション開発の流れそれでは、gRPCを使ったアプリケーション開発を行う場合、実際にどのような手順を踏めば良いかを紹介していこう。この場合の基本的な流れは次のようになる。Protocol Buffersを使ったサービスの定義サービス定義ファイルからのコードの生成生成したコードに独自の自前の実装を追加する上記に関してはハンズオンなどで実施していきたいと思います。また、2022年4月27日に「Protocol Buffers/gRPC を安全に書き進めるためのエトセトラ」と題してOWASP Fukuoka Meeting #6にて登壇いたしますーowasp-kyushu.connpass.com死霊👻 はこちらです speakerdeck.com参考文献公式資料grpc.iogRPC の公式サイトです。仕様だけでなく、各言語のチュートリアルもあります。grpc.github.io詳細なドキュメント群です。gRPC over HTTP2上記サイトの一ドキュメントです。HTTP/2 をどう利用しているかの仕様書です。developers.google.com/protocol-buffersProtocol Buffers の公式サイトです。The complete gRPC courseGoとJavaで開発できるチュートリアルです。gRPC: Up and RunninggRPC と Protocol Buffers の本です。Securing your gRPCApplicationKubeCon 2019 NA のセッションの一つで、gRPC の認証・認可の実装方法を詳しく解説しています。","link":"https://syu-m-5151.hatenablog.com/entry/2022/04/12/130411","isoDate":"2022-04-12T04:04:11.000Z","dateMiliSeconds":1649736251000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"samber / lo はGoperである私を愚直なfor もしくは筋肉信仰から救ってくれるのか？","contentSnippet":"はじめにGo 1.18 がリリースされました。Go 1.18でシュッとGenerics を手軽に良さを実感する方法としてsamber/lo があります。  もちろん、Tutorial: Getting started with generics で完全に理解できるならそちらの方が良いですし、これを終わった後でやることも推奨です。その他のリリースパーティや勉強会もとても勉強になりますが とにかく、samber/lo 便利なので紹介させてください！！！！！！！go.dev今回はとても大きな変更です。Generics が入りました。構成としては2つ。Type parameterType sets参考資料DevFest Tokyo 2021 でmattn さんが発表したスライド＆動画がとても分かりやすいので是非、見てみてください。docs.google.comwww.youtube.com全てfor 文で解決するのか？- そう、全て筋肉が解決してくれるGolang にはuniq メゾットのようなものがなく、重複のある slice に対して独自に処理を実装しなければいけなかった。愚直にfor を回すの結果として最速だからである。arr := []string{\"Samuel\", \"Marc\", \"Samuel\"}m := map[string]bool{}for _, ele := range arr {    if !m[ele] {        m[ele] = true        uniq = append(uniq, ele)    }}fmt.Printf(\"%v\", uniq) // [\"Samuel\", \"Marc\"]Go Playground - The Go Programming Languageどういうことかというと、重複キーがあるので、同様のキーを持つmapの場合は新しく値を上書きしないみたいな処理を書かなければならなかった。m[\"Samuel\"] = true は一度目はこれが呼ばれるけど、二度目はすでにtrueなので if句の中に入ってず、resultにSamuelが二度入ることがないという様な仕組みです。とにかく、全てをfor で扱い全ての型を制御するマッチョでした。ema-hiro.hatenablog.com全てfor 文で解決するのか？- samber/lo とか？Golang にはuniq メゾットのようなものがなく、重複のある slice に対して独自に処理を実装しなければいけなかったがsamber/lo というプロジェクトではGo 1.18 のGenerics を使うことによってreflect より早くforとも遜色なく動作するヘルパーを提供します。他にもいくつもの ヘルパー がありますが今回はuniq のみ紹介します。pkg.go.devpackage mainimport (    \"fmt\"    \"github.com/samber/lo\")func main() {    arr := []string{\"Samuel\", \"Marc\", \"Samuel\"}    names := lo.Uniq[string](arr)       fmt.Println(names) // []string{\"Samuel\", \"Marc\"}}uniqValues := lo.Uniq[int]([]int{1, 2, 2, 1})// []int{1, 2}実装をみるとこんな感じでmapと空のstructを使う方法でuniq が実装されている。lo/slice.go at v1.10.1 · samber/lo · GitHubfunc Uniq[T comparable](collection []T) []T {    result := make([]T, 0, len(collection))    seen := make(map[T]struct{}, len(collection))    for _, item := range collection {        if _, ok := seen[item]; ok {            continue        }        seen[item] = struct{}{}        result = append(result, item)    }    return result}とにかく、for で愚直に回す言語から多少はスマートな解決ができる様になった(もしくは今後、期待ができる様になった)。最後にこの記事を読んで興味が湧いたら元のProposalやTutorial: Getting started with generics を読んでみてください。自分も何度かやってみて読んでみて使える様になりたいと思ってます。また、Go本体にも機能として追加される日を楽しみしてます。github.com","link":"https://syu-m-5151.hatenablog.com/entry/2022/03/16/122810","isoDate":"2022-03-16T03:28:10.000Z","dateMiliSeconds":1647401290000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"3-shake エンジニアのブログ記事まとめサイト「3-shake Engineers' Blogs」を公開しました。","contentSnippet":"3-shake Engineers' Blogs、爆誕す！3-shake inc. に所属するエンジニアが執筆したブログ記事をまとめたサイト、3-shake Engineers' Blogsを公開しました。blog.3-shake.comこちらは、@catnoseさんがOSSとして公開している、team-blog-hubをfork させていただき、Ubie さんのUbie Engineers' Blogsを参考にして作成いたしました。なぜ、作ったのか？3-shake には現在、公式のテックブログ(Sreake のブログ | sreake.com | 株式会社スリーシェイク というブログ)があります。が、メンバーが自発的にブログをポストしているわけではありません(別に良い悪いではなく)。理由はいろいろあると思いますが、テックブログは続かない - 何サイトか潰した後にブログが有名な企業に転職しての気づきと反省｜久松剛／IT百物語の蒐集家｜note にあるようないくつかの要素が原因かと思っています。が、3-shake がアウトプットしない文化という訳では決してありません。3-shake には現在、Sreake共有会 という毎週、火曜日と木曜日に担当者が現場で得た知見や調査した内容を発表する社内勉強会が開催されておます。これのポストは既に100件近く内部資料として溜まっており、レベルも相応に高いです。それらを対外的なアプトプットとして出せて、かつ、個人のブログでアウトプットしたほうがアウトプットするモチベーションも上がるのでは？という考えのもとに作成いたしました。最後にこれらの取り組みが3-shake を知っていただけることに多少なりとも繋がれば良いと思います。ちなみに、リポジトリをfork した後に社内調整をして、公開までいたしました。社会人力の低さを感じましたが3-shake が大切にしている価値観として5倍速というのがあるので許される気がしてます。@nwiizo さんのご尽力もあり、流行りに乗っかってみました笑うちのメンバーのブログをぜひ見てみてくださいー— TakuyaTezuka@3-shake (@tt0603) 2022年3月15日  告知また、3-shake で働くことに興味がある方は、採用サイトやホームページに詳しい情報を掲載していますのでご覧くださいwww.wantedly.com今週の金曜日の2022年3月18日に 3-shake SRE Tech Talk #3 というイベントがあって技術顧問 のまつもとりーさんが「コンテナの研究開発から学ぶLinuxの要素技術」と題してお話してくれるので皆様にも参加してほしいです3-shake.connpass.com参考資料zenn.devnote.com","link":"https://syu-m-5151.hatenablog.com/entry/2022/03/15/153309","isoDate":"2022-03-15T06:33:09.000Z","dateMiliSeconds":1647325989000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"Observability Conference 2022 に登壇しました","contentSnippet":"「Dapr の概念と実装から学ぶ Observability への招待」 というタイトルで登壇します。https://event.cloudnativedays.jp/o11y2022/talks/1382:embed:cite セッション概要Dapr は CloudNative な技術を背景に持つ分散アプリケーションランタイムです。本セッションでは Dapr の Observability に関する各種機能と、その実装について解説していきます。さらにスリーシェイクの Dapr と Observability への取り組みに関してもご紹介します。Dapr の機能でカバーできる点...","link":"https://zenn.dev/nwiizo/articles/d837b78914de23","isoDate":"2022-03-11T04:02:18.000Z","dateMiliSeconds":1646971338000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"Kind を利用してFeature Gates のGRPCContainerProbe を有効にしたKubernetes クラスターを構築してアルファー機能のgRPC Health Checkを試したいなー","contentSnippet":"Kind でGRPCContainerProbe がやりたいよはじめにKubernetesではLiveness & Readiness Probeを使って、Pod内のコンテナ、プロセスのヘルスチェックが行える。Kubernetes上で動くgRPCサーバーのヘルスチェックする際にはgrpc-health-probeで簡単に実装できます。readinessProbe やlivenessProbe,startupProbeにexec のcommand として実装する必要がある。どういうような方式があってみたいなのはHealth checking gRPC servers on Kubernetes | Kubernetes を参照していただければと思います。現行の場合にはこのように設定が必要  containers:  - name: server    image: \"[YOUR-DOCKER-IMAGE]\"    ports:    - containerPort: 5000    readinessProbe:      exec:        command: [\"/bin/grpc_health_probe\", \"-addr=:5000\"]      initialDelaySeconds: 5    livenessProbe:      exec:        command: [\"/bin/grpc_health_probe\", \"-addr=:5000\"]      initialDelaySeconds: 10tomioka-shogorila.hatenablog.comgRPC health checking が alpha feature として追加この、方式ではDockerfile内 にgrpc_health_probeを入れなくてはいけません。で、2022年2月で最新のKubernetes v1.23 では built-in gRPC health checking が alpha featureとして追加されました(同僚に教えてもらいました)。kubernetes.ioそのため、Kubernetes上で動くgRPCサーバーのヘルスチェックする際にbuilt-in でできるようになりました。  containers:  - name: server    image: \"[YOUR-DOCKER-IMAGE]\"    ports:    - containerPort: 5000    readinessProbe:            grpc:              port: 5000    livenessProbe:            grpc:              port: 5000しかし、これらの機能はまだ、alpha feature で機能 です。なので、defaultでは無効です。もし、試したい場合には--feature-gates として有効にしてあげればならないkubernetes.ioローカルでKubernetes クラスターを構築するにはいくつか方法があるのですが今回は、Kind を利用しているので今回もこちらを利用する。Kind でFeature Gates を利用するにはyaml で以下のように true にすることで適応できる。今回はGRPCContainerProbeをtrue にすれば良い。kubeadm でできることはKind でも大体できるのでkubeadmConfigPatches みたいな設定も忘れたくない。kind: ClusterapiVersion: kind.x-k8s.io/v1alpha4name: featuregatesnodes:- role: control-plane  image: kindest/node:v1.23.3@sha256:0df8215895129c0d3221cda19847d1296c4f29ec93487339149333bd9d899e5afeatureGates:  GRPCContainerProbe: truegithub.com最後にO11yCon での資料作成があるのにブログを書いてしまった。試験前に漫画を読み始めるみたいな感じでブログを書いてしまった。13歳からやってることは変わりません。お時間があれば見に来てください。event.cloudnativedays.jp","link":"https://syu-m-5151.hatenablog.com/entry/2022/02/22/112713","isoDate":"2022-02-22T02:27:13.000Z","dateMiliSeconds":1645496833000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"CloudNativeな時代に求められるWebサービス基盤モデルの再考 というタイトルで Developers Summit 2022 に登壇しました。  ","contentSnippet":"振り返りDevelopers Summit 2022 で登壇しました。CloudNativeな時代に求められるWebサービス基盤モデルの再考 - Daprについての考察と実装 というタイトルで、CloudNativeな技術と共に歩んできた中で見えてきた、CloudNativeな技術を背景に持つ分散アプリケーションランタイムであるDaprがどういったもので何ができるかを解説するのを通してCloudNativeの必要性や立ち位置について発表しました。event.shoeisha.jp今回の発表でもっと喋りたいと思ったのはDapr によって Complexity は解消されるのか？に関する部分でRepositoryが何をするのかとDapr になります。実は来週もDapr での登壇イベントがあるのでRepository 周りの話と実装面の話を重点的にさせていただきたいと思います。jjug.doorkeeper.jp登壇資料体調が悪いのに頑張って資料作りましたね。偉いです。 speakerdeck.com文字起こししておいてます。意味はないです。振り返り登壇資料自動化とコンテナの話Infrastructure as Document 時代システム運用担当者がアプリケーションを配置する方法Infrastructure as Code 時代システム運用担当者がアプリケーションをコードによって配置する約束された勝利の自動化なんてない自動化の不都合な真実最初はうまくいく腐らないようにする自動化Immutable Infrastructure自動化を進めていく中で発見された素晴らしいプラクティスコンテナへの道アプリに必要なものを全て特定のフォーマットで固めて、展開するだけで起動自動化とコンテナのざっくりとした関係インフラの腐敗を防止クラウドによるアプリケーションファーストクラウドにより組み上げ方式から呼び出し方式へコンテナによって取り扱いを共通化Cloud NativeとWebサービス基盤モデルについて再考Service Level indicator とService Level Objective信頼性 100%と、信頼性 99.99%では大きな違い各サービスごとに適切なSLOを設定することが大事Service Level Indicator の例シングルノード100 台でアプリを動かすと…　ｱｯｱｯｱｯCloud Native とは？疎結合ではないシステムとは？AvailabilityScalabilityComplexity疎結合なシステムとは？ResilientFlexible ScaleSimplicityKubernetes の特徴不変なインフラ宣言的設定自己修復機能Webサービス基盤モデルについてサービス層ストラテジー層オーケストレーション層コンテナランタイム層インフラストラクチャ層ストラテジー層の進化と拡大istioKnativeストラテジー層の拡大とDapr についてDapr とはComplexityとDapr の実装についてDapr とは？Daprの特徴Goalサイドカー パターンとは？分散システムにおけるデザインパターンの一つDapr におけるサイドカーアーキテクチャDapr の多様性ビルディングブロックビルディングブロックコンポーネントRepository において Dapr による抽象化の理想と現実Repository とはDapr によるRepository の吸収しきるのか？何を抽象化しているのか？Dapr によって Complexity は解消されるのか？Complexity とはDaprでComplexity の解消の夢は見れるか？参考資料自動化とコンテナの話Infrastructure as Document 時代システム運用担当者がアプリケーションを配置する方法人間の思考や行動をコーディングしてシステムを管理する為の秘伝のドキュメントを引き継ぐ一台一台を丁寧に設定していくが出自は不明なぜ、動いているかも分からず、さわったら動かなくなるから秘伝のタレとなりいつか腐るDocument はあるが更新されてないこともしばしばInfrastructure as Code 時代システム運用担当者がアプリケーションをコードによって配置する直訳すると「コードとしてのインフラ」、「インフラをコードで記述する」ことさまざまな手順や前提をCodeの中で表現していくことで全てが見えてくる黎明期はシステム管理の自動化がその後はソフトウェア開発プラクティスを応用するのが焦点に約束された勝利の自動化なんてない自動化の不都合な真実自動化も腐りやすい最初はうまくいくOS やミドルウェアのバージョンを上げると死ぬアプリ毎、ホスト毎に個別化、属人化やシステムの複雑化が進行自動化でトラブルので手作業が多くなっていく触るのが怖くなったら秘伝のタレが腐敗している合図腐らないようにする自動化約束されてる勝利の自動化は実は部分的Immutable Infrastructureインフラを管理する手法の一つで「一度構築した本番環境には更新やパッチの提供などの変更を加えず稼働させる」というような考え方自動化を進めていく中で発見された素晴らしいプラクティス常にクリーンインストールから開始必要なものは全て固めアプリを共存させないコンテナへの道アプリに必要なものを全て特定のフォーマットで固めて、展開するだけで起動2013年に最初にリリースされ、Docker,Inc.によって開発Dockerは「コンテナ」と呼ばれるソフトウェアパッケージを実行される起動の約束された勝利のアプリケーションベースイメージをダウンロード必要な各種ソフトウェアは全てコンテナ内にインストール必要な設定はコンテナ作成時に仕込む自動化とコンテナのざっくりとした関係同じモノとして扱いやすくなるインフラの腐敗を防止運用が統一的開発でテストし、そのままを運用に適用可能環境の影響を受けずに自動化の負担は軽減クラウドによるアプリケーションファーストクラウドにより組み上げ方式から呼び出し方式へ要求すれば下位のリソースが自動的に割り当たるコンテナによって取り扱いを共通化アプリとインフラの依存関係を断ち切ることができるCloud NativeとWebサービス基盤モデルについて再考Service Level indicator とService Level Objective信頼性 100%と、信頼性 99.99%では大きな違い信頼性 100%を実現するためには、99.99%とは異なり膨大な工数を投入が必要ほとんどのユーザーにとっては「99.99%」が「100%」になったからといって、大きなメリットがあるわけではありません🥺🥺🥺各サービスごとに適切なSLOを設定することが大事SLOとは、SLIで計測されるサービスレベルの目標値、または目標値の範囲を指しますSLOを「年99.99%」と設定すると、「1年のうち52分は稼働しなくてもよい」ということになりますしかし、シングルノード100 台でアプリを動かすと…　ｱｯｱｯｱｯService Level Indicator の例リクエストのレイテンシ（リクエストに対するレスポンスを返すまでにかかった時間）エラー率（受信したリクエストを正常に処理できなかった比率）システムスループット（単位時間あたりに処理できるリクエスト数）可用性（サービスが利用できる時間の比率）シングルノード100 台でアプリを動かすと…　ｱｯｱｯｱｯクラウドを使ったからといって…可用性が勝手に高まるわけではないシングルノードでの運用について* アプリの更新は？* サーバー自体が電源断でサービス落よね？* 負荷が増えたらどうするの？* アクセスが増えたらどうするの？何らかの方法でリスクを回避せねば、その方法の一つがKubernetesでありCloud Native となるCloud Native とは？github.comコンテナサービスメッシュマイクロサービスイミューダブルインフラストラクチャおよび宣言型APIなどを用いて回復性管理力可観測性堅牢な自動化によって変化に強い疎結合なシステムを実現する疎結合ではないシステムとは？Availabilityコンポーネントが死ぬと全体が死ぬScalability一つの機能をスケールさせるためには全体のスケールが必要Complexity新しい機能を追加するときに全体との調和が必要なので大変疎結合なシステムとは？Resilient一つのサービスが死んでも一部のサービスは継続Flexible Scaleサービスごとに独立してスケールリソースの最適化Simplicity小さな単位で開発することにより新機能の追加が容易になるKubernetes の特徴不変なインフラ一度、構築したインフラは変更を加えることなく破棄して、新しいものを構築しなおせばよいImplicit or Dynamic Grouping(入れるところないのでココに書いておきます)宣言的設定命令的に手順や変更履歴を記録するのではなく宣言的な設定ではシステムのあるべき姿を定義します。Kubernetesはこの定義ファイルを確認してあるべき姿に自律的に動作するDeclarative Configuration自己修復機能Kubernetesが障害や異常があった時にあるべき姿になる為にシステムが設定した通りにAPIを再起動したり様々な作業を自動で行うReconciliation Loopにて実現Webサービス基盤モデルについてサービス層実際のWebアプリやWebサービスのコンテンツ層ストラテジー層Webサービスの特性に合わせてコンテナ基盤をより特徴的に制御する層オーケストレーション層コンテナ群や収容ホスト群のモニタリングやリソース管理等によってCRIを介してコンテナを制御する層コンテナランタイム層コンテナそのものの制御層インフラストラクチャ層ハードウェアやVM、ベアメタル等のコンテナのリソースプールを実現する層ストラテジー層の進化と拡大istioマイクロサービスアーキテクチャにおけるネットワーク面での課題を解決する機能群を提供するKnativeモダンなサーバーレスワークロードをビルド、デプロイ、管理するためのKubernetesベースのプラットフォーム Knative など、さまざまな用途のアプリが誕生しているストラテジー層の拡大とDapr についてDapr とは効率的なクラウドネイティブアプリ開発を目指した分散アプリケーションランタイムDapr は サイドカーによりサービス間の呼び出し、ステート管理、サービス間メッセージングなどの非機能要件を実現する事で分散アプリケーションの実装上の課題を解決する機能群を提供するフレームワークです。非機能的ではあるが本来、サービス層が持っていた一部機能をストラテジー層が担っている。ComplexityとDapr の実装についてDapr とは？Distributed Application Runtime の略Daprの特徴サイドカーにより任意の開発言語やフレームワークで開発可能ベストプラクティスをビルディングブロックとして提供Goalあらゆる言語やフレームワークを使用して、分散アプリケーションを記述することが可能ベストプラクティスのビルディングブロックを提供することで、マイクロサービスアプリケーションの構築で開発者が直面する困難な問題を解決コミュニティ主導で、オープンかつベンダーニュートラルであること新たな貢献者の獲得オープンAPIによる一貫性とポータビリティの提供クラウドやエッジなど、プラットフォームにとらわれないベンダロックインすることなく、拡張性とプラグイン可能なコンポーネントを提供する高いパフォーマンスと軽量化により、IoTやエッジのシナリオを可能にする実行時に依存することなく、既存のコードからインクリメンタルに採用できるサイドカー パターンとは？分散システムにおけるデザインパターンの一つサイドカーは、アプリケーションコンテナを拡張および拡張して、機能を追加します。サイドカーを使用して既存のレガシーアプリケーションなどにも適用できます。同様に、これらを使用して、一般的な機能の実装を標準化するコンテナを作成することもできます。Dapr におけるサイドカーアーキテクチャDapr の多様性Dapr サイドカーにより、HTTP/gRPC での通信が可能であれば開発ができる公式SDKも提供されているビルディングブロックビルディングブロック一般的にはシステムアーキテクチャを構成する要素Dapr では利用可能な機能群のことを指す場合が多いマイクロサービスのベストプラクティスを体系化して機能として実装されてるサイドカーのHTTP/gRPC を呼び出してこれらを利用することができる2022年2月で 8つのビルディングブロックが用意されているサービス間呼び出し状態管理パブリッシュとサブスクライブバインダーアクター可観測性シークレットの管理構成設定コンポーネントビルディングブロックで利用される機能モジュール一つ以上の複数のコンポーネントを使用可能IF が用意されているのでこれらに合わせて機能を実装統一されたエンドポイントが利用できるのでアプリ側に複雑性を抱え込まなくて良いRepository において Dapr による抽象化の理想と現実Repository とはDDDのレイヤードアーキテクチャで提唱されいるRepository でインターフェースを定義することによりInfra層を抽象化、依存性の逆転モックの差し替えが可能になり、Application層のユニットテストが可能になるDapr によるRepository の吸収しきるのか？何を抽象化しているのか？Repository は、抽象化とレイヤー化を同時に行うのが一般的Dapr は前述したビルディングブロックと公式のSDKによってプロトコルの抽象化が可能抽象化に合わせた実装を一部しなくても良いので全体の実装量はそりゃ、減るチームでどのような実装にしていくか話し合いが必要であり、レイヤー化に関してはあまり寄与しないDapr によって Complexity は解消されるのか？Complexity とは認識や変更を困難にするソフトウェアの構造に関する全てのものを指しますどれだけ実装が「複雑」でも開発者が読み書きする必要がないようになっていれば、それはComplexityとは言いませんProxy が担う部分はまだ、機能がまだ少ないDaprでComplexity の解消の夢は見れるか？Dapr によって抽象化の一部のメリットは得られるDapr でもレイヤー化するのは自分達であることを忘れずにMockClient みたいな話がgo-sdk でも出ればいいが特にないので自分達で用意する必要がある参考資料Dapr Docshttps://docs.dapr.io/Infrastructure as Codeのこれまでとこれから/Infra Study Meetup #1 よりhttps://forkwell.connpass.com/event/171560/ふつうのLinuxプログラミング 第2版　Linuxの仕組みから学べるgccプログラミングの王道https://www.sbcr.jp/product/4797386479/コンテナ時代のWebサービス基盤モデル - FastContainerの研究発表をしてきましたhttps://rand.pepabo.com/article/2017/06/28/iot38-matsumotory/目的に沿ったDocumentation as Codeをいかにして実現していくか / PHPerKaigi 2021https://speakerdeck.com/k1low/phperkaigi-2021クラウドネイティブとKubernetes（だいたいあってるクラウドネイティブ）https://speakerdeck.com/hiro_kamezawa/kuraudoneiteibutokubernetes-daitaiatuterukuraudoneiteibuDesigning Distributed Systems (PUBLISHED BY: O'Reilly Media, Inc.)https://learning.oreilly.com/library/view/designing-distributed-systems/9781491983638/ボトムアップドメイン駆動設計https://nrslib.com/bottomup-ddd/Repositoryによる抽象化の理想と現実/Ideal and reality of abstraction by Repositoryhttps://speakerdeck.com/sonatard/ideal-and-reality-of-abstraction-by-repositoryA Philosophy of Software Designhttps://web.stanford.edu/~ouster/cgi-bin/book.php","link":"https://syu-m-5151.hatenablog.com/entry/2022/02/17/182336","isoDate":"2022-02-17T09:23:36.000Z","dateMiliSeconds":1645089816000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"さよなら、俺のVim。Neovim への旅立ち","contentSnippet":"はじめにこんにちは、nwiizo です。私は今から10年前にVim を使い始めました。Vimはviから発展したテキストエディタです。コード補完、コンパイルまたエラージャンプなど、プログラミングに特化した機能が豊富で、広くプログラマに使用されています。私も思考のスピードでの編集をvim で実践してきた1人でした。2022年の現在ではVim vs Emacsなど言われていた時代も遠い過去になり、VSCode１強になりつつあります。そういう、私も少ない設定で動作する強力で最高のJetBrainsやVSCode に浮気をした回数は数え切れません。IDEの生産性を身に染みて感じながらも、身についた操作感/キーバインド及びターミナルからの起動の速さが辞められず。しかし、vimrc を強力に設定しているわけでもなく愛の力のみで心の擬似IDEとしてのvimを使っておりました。進まねばならぬこのままでは愛に沈む。愛に殉じたいが痛いおじさんになりたくない。意を決してVim からNeoVimへの移行を決めました。ふと、NeoVim に環境を移すことを決めました。— nwiizo (@nwiizo) February 7, 2022  俺のvimrc過去にはこんなことを言ってた。やってること変わってない。syu-m-5151.hatenablog.comgithub.com初期構想をやめましたNeovim + coc.nvim + (Neo)vim Plugin で初期構想を考え手を動かしてましたが、結果として断念しました。理由として、今夜中に変更したかったこと。既存のプラグインに、そんなに力を入れていなかったこと。深夜テンションで入れ替えを行なった為に、下調べが足らずにプラグインの選定や大量に入れたプラグインの起動時間の短縮などがめっ… 難しかったからです。起動時間を短縮しようとしてる様ですNeoVim 感情のままplug-in 入れてそのまま起動すると全然、立ち上がらない。生活を考えてから設定作らないとだめ。— nwiizo (@nwiizo) February 8, 2022  こちらのドキュメントは非常に参考になりました。ありがとうございます。zenn.dev選ばれたのは、よい設定を求めてインターネットをさすらっているとvim-config なるリポジトリに出会いました。欲しかったプラグインがほとんど入っており、何より先ほどまで苦戦していた起動時間が短いという単語に惹かれてすぐに入れて動かしてみました。github.comvim-configがどのようなプラグインや設定を使ってどのように設定を実現させているかやいくつかのショートカットについてはリポジトリで確認お願いします。おそらく、それだけでかなり、勉強できるのでおすすめです。使用感は最高でプラグインのショートカットをいくつか試したりコードを書いたりしました。また、各言語の設定については~/.config/nvim/config/local.plugins.yaml などに設定を入れておくと良いですが。私は、vim-goを入れました(がのちに削除)。:GoDef による定義位置ジャンプは vim-lsp が有効になっていれば gd で可能です(sbで前のbufferに戻れるのでそれで行き来できる。)。公式ドキュメントの設定に従ってImports を設定する。その内にlspの自動補完が上手くいってないことに気付いたので色々設定を見ていき~/.local/share/nvim/ などの共有設定に阻害される設定が入っていたので移動させた結果、無事にlsp 自動補完も動作しました。~/.local/share/nvim/ などの既存設定file が邪魔してないかを確認して.config/nvim/config/local.plugins.yaml にvim-go を入れたらちゃんと動きました— nwiizo (@nwiizo) February 7, 2022  さいごにこれが大好きだったvim との別れです。今日からはこれでコーディングしてみたいと思います。大好きだったvimが強くなり、帰還した。そういう、感じがして今日はとても素敵な気分です。朝会で共有したらVSCodeではないのかって笑いが起きました。ちゃんと前に進んでいるいい職場です。私のnvim の設定をこちらに置いておきます。現在はこちらでいくつかのlspを入れて開発しております。github.com","link":"https://syu-m-5151.hatenablog.com/entry/2022/02/08/130305","isoDate":"2022-02-08T04:03:05.000Z","dateMiliSeconds":1644292985000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"『Learning Dapr』を読んだので感想文","contentSnippet":"『Learning Dapr』を読んだ。輪読会ではなく共有会としての読書感想文を記載しました。Dapr に関する素晴らしい書籍だと思うので共有会を通して皆さんに共有したいと思います。誰が読むべきか？Dapr の実装が必要な方分散システムに関して兎に角見聞を広げたい方Learing Dapr についてlearning.oreilly.comどんな本なのか？Daprがステートレス、ステートフル、アクタープログラミングモデルを統合するだけでなく、クラウドやエッジのあらゆる場所で実行される方法を背景からその将来について開発者自ら、説明してくれている書籍。話の主軸がDaprであるためどうしても、Daprを利用する以外では他の書籍を読んだ方が効率が良いというか理解しやすいと思う。分散システムの実装に関して別の観点から理解を深めたければ読めば良いと思うまた、どんな本ではないのか？分散システムを学べる本ではないKubernetes が学べる本ではない主要なコンテナオーケストレーターであるKubernetesを本番環境で動かすために役立つベストプラクティスやKubernetes の概念が学べる本ではない。Production KubernetesGolang に入門できる本ではないGolang の入門的な内容はなく各章ごとに適当な言語で書かれているThe Go Programming Languageマイクロサービスに関する本ではないマイクロサービスのためのシステム分割や設計のレベルから解説する本ではない。あくまでdapr のための書籍であってシステム分割や設計が前提としてマイクロサービスパターン 実践的システムデザインのためのコード解説 - インプレスブックスDapr とはそもそも、Dapr とは、Microsoftが中心になって開発しているOSSの分散アプリケーションランタイム、Distributed Application Runtimeの略でDaprです。Dapr は様々なクラウドサービスやミドルウェアを良い感じに透過的に扱うことを目的としたプロダクトで、なかなか筋が良いのですが、何に使えるかよく分からないというか、そもそもどういうものか分かりづらいので今回はオライリーから出版されたLearning Dapr を年末に読んだので共有会で共有していこうと思います。スター数が15K とかなりいきおいのあるOSSのプロジェクトではないかと思いますDaprの特徴Dapr はサイドカーとして利用することで。本来実装したいコアロジックに集中でき簡単にマイクロサービスを作成することができます。また、デプロイする環境はKubernetes もしくはローカル環境を選ぶことが来ます。更にそれぞれのビルディングブロックは抽象化されており、 HTTP/gRPC API を通して利用するものとなっているため言語に縛られない開発ができるのも魅力となっています。それぞれのコンポーネントはライブラリとしてアプリケーションに組み込むのではなく、yamlのコンポーネント定義ファイルをロードさせることで利用することができるので実装に一切手を加えず、検証環境ではredis、本番環境では何かしらのクラウドサービスなど切り替えが可能。Service-to-service invocation: /v1.0/invoke他のマイクロサービスサービスへ通信するための機能State management: /v1.0/statekey/valueベースの永続化や参照機能Publish and subscribe: /v1.0/publish and /v1.0/subscribePublish/subscribeモデルで非同期にメッセージを送受信する機能Resource bindings: /v1.0/bindings外部コンポーネントやサービスを抽象化しイベントの送受信を行う機能Actors: /v1.0/actors分散性や並行・並列性をもち、非同期なメッセージ駆動のアクターモデルを提供Observabilityログ・トレース・メトリクス・ヘルスチェックといったオブザーバビリティに必要な要素を提供(この辺の情報は整理してObservability Conference 2022にCfPに投げます)Secrets: /v1.0/secrets安全にパスワードなどのクレデンシャルなデータにアクセスする機能Extensible拡張性に優れたミドルウェアRate limit やOAuth2 、Open Policy Agent などさまざまな機能をミドルウェアとして実装可能となっています。  components-contrib/middleware/http at master · dapr/components-contribサポートしてるSDKdaprが提供する HTTP/gRPC API にアクセスするためのsdkを利用することもできます(クライアント同士の直接参照も可能なので)。提供されているSDKは、以下の8つの言語になります。Java-sdkPython-sdkDotnet-sdkJs-sdkGo-sdkCpp-sdkPHP-SDKWIP: Rust-sdk今後、サンプルを書く際にはGo-sdk を利用して書いていくこととします。Dashboard個人的に気に入っている機能としてはダッシュボードの存在があります。Kubernetes に対しても実行できるので非常に重宝をしております。https://github.com/dapr/dashboard/blob/master/docs/development/changelog.mdやっと、目次全7章から成り立っています。どの章も公式ドキュメントよりも背景であったりとかメタ情報が付与されており1. Services2. State3. Messaging4. Security5. Actors6. Application Patterns7. Dapr’s Future1. ServicesDapr 対応アプリケーションの基本的な単位はサービスと呼ばれます。この章では各サービスの状態管理やトレース、必要なときに安全な通信様々な機能がどのように使えるのかを簡単に説明した章になります。この章までであれば公式ドキュメントを読めば大体なんとかなる2. StateDapr の状態管理は、プラットフォームに依存しないクリーンな状態処理コードを記述しながら、これらの課題に対処するのに役立つ単純な状態APIを提供することを目的としています。状態管理についてクラウド化や様々なデータストアにどのように対応していくかどういった方法が取れるかなどが記載がされております。3. MessagingDapr は pub/sub にKafka やRabbitMQなどをバインディングすることが可能で一般的なメッセージング構造を提供しますが、独自のメッセージングバックボーンを作成しません。Dapr の pub/subを使用すると、メッセージパブリッシャーはトピックにメッセージをパブリッシュでき、トピックのすべてのサブスクライバーはメッセージのコピーを取得できます。Daprは、メッセージが少なくとも1回処理されることを保証します。上記で説明したようにDaprはpub/sub に外部リソースのバインディングが可能でそれらをもとにシステムを作成することができます。この章ではそういったdaprの機能や特性を活かしてどのようなアーキテクチャが考えられるかについて書かれた章4. SecurityDaprは追加の機能としてではなくデフォルトで提供します。それらがどのように実装されているかという記載はないですがどうやって実装していくかについて記述がされている。Daprは、シークレット管理、シークレットAPI、相互TLSサポートなどの基本的なセキュリティ機能のセットを提供しています。いくつかの追加のセキュリティ機能のIssueや が挙げれらている。5. ActorsDaprは、クラウドネイティブで復元力のあるプラットフォームに依存しない仮想Actorモデルを提供します。ActorランタイムはDaprランタイム内で実行されるため、Dapr上に言語固有のアクターSDKを簡単に記述でき、他の機能と同様にHTTPまたはgRPCを介して任意の言語からアクターを呼び出すことができます。この章ではActor によるターンベースの同時実行、状態管理、Timer やreminderなどの独自の機能を説明している。Daprがアクターインスタンスを独立したプロセスとしてではなく、同じWebサービス上のルーティングルールとして扱うことです。これにより、Daprは高密度でアクターインスタンスをホストできます。6. Application Patternsdaprのアプリケーションコードは、さまざまなイベントソースからのイベントに応答し、コネクタを介して他のシステムにイベントを送信/受信できます。クラウド環境でどのように利用できるのか？既存のサービスメッシュとどのように協業するのかについて記載がされている章単発で為になる記載もいくつかあったが何回読んでも難しすぎて参考文献になりそうなものを読んだりして行ったり来たりしてる7. Dapr’s Futureシェルスクリプトとしてのdapr の実行やWebAssembly,アクターの各種機能などDapr が将来どのように成長するかを開発者自身が書いてくれている章","link":"https://syu-m-5151.hatenablog.com/entry/2022/01/18/124731","isoDate":"2022-01-18T03:47:31.000Z","dateMiliSeconds":1642477651000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"2021年9月 の労働などの振り返り","contentSnippet":"登壇理論と実践からSREを再考するSRE Gaps「理論と実践からSREを再考する」で座談会のファシリテーションを行いました。過去に翻訳刊行された『SRE サイトリライアビリティエンジニアリング』『サイトリライアビリティワークブック』を読んで、SREに取り組む企業が増加しました。がこれらの本で言及されている事例はほとんどがGoogleの事例でした。今月『SREの探求』という本が出ました。この本は、Google以外の会社がSREを導入実践する内容がメインになっております。本のリリースの記念したイベントでもあります。また、本の内容については今後、文章にしていきたいと思ってます(第1章がコンテキストに関する内容でまとめるのめちゃくちゃ難しい)。forkwell.connpass.comsyu-m-5151.hatenablog.com来月は所属組織であるスリーシェイクのイベントでGo言語の可観測性に関する内容で登壇しようと思います。3-shake.connpass.com現状では本を書いてたり書くための技術検証を行なっているので登壇資料に落とし込むかーっと思うなどしてます。次の技術書展は本を出したいと思っているのですが何からはじめればいいんですかね？— nwiizo (@nwiizo) 2021年7月8日  3-shake.com での仕事2021年6月1日に転職して丸4月経過しました。とても、学びと悪戦苦闘の日々を送っており、所属としてはSreake事業部になります。私たちはSREのプロフェッショナルパートナーですSreake（スリーク）は、金融・医療・動画配信・AI・ゲームなど、技術力が求められる領域で豊富な経験を持つSREの専門家が集まったチームです。戦略策定から設計・構築・運用、SaaS提供まで、幅広い領域をサポートします。sreake.com現職では支援事業を行なっており、さまざまなプロジェクトに参加しておりますが、これらが喋っていい内容かどうかは不明なので詳しい技術に関する話はしません。SRE支援事業の感想SREの支援事業をしていて思うことはSREの導入に成功している企業は、Googleが提唱しているSREの概念を理解しつつ、自社の状況やプロダクトのフェーズにあわせて常にSREのあるべき姿を変化させていることです。「SREはこうでなくてはいけない」という固定概念にとらわれず、常に柔軟に変化する意識を持つと良いのではないかと思っている。 変化するものだけが勝つとか誰も言ったことないようなこと言いたいと思います。SREはDevOpsというinterfaceの実装であるまた、SREを実装するうえで技術的な要素はもちろん大切ですが、SREの組織として「どのようにすればうまく状況が共有されるのか」「どのようにすれば横断的な運用管理ができるのか」を考えて行動することも大切ではないかと思います。技術のみに囚われすぎず、定期的なミーティングや組織体制の見直しを行うことで、より良い体制作りができるのではないかと夢想してました。","link":"https://syu-m-5151.hatenablog.com/entry/2021/10/01/171059","isoDate":"2021-10-01T08:10:59.000Z","dateMiliSeconds":1633075859000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"「理論と実践からSREを再考する」というイベントで司会進行とパネルディスカッションのモデレーターをやる #SREGaps","contentSnippet":"はじめに本日、2021年9月9日19時30分より『SRE Gaps「理論と実践からSREを再考する」』というイベントがあります。私、nwiizoはそこでパネルディスカッションのモデレーターを行うことになりました。forkwell.connpass.com500名以上も参加登録されていてかなり注目されていると思います。SRE という概念を導入する日本企業の開発組織も増えてSREの考え方は多くの場所で取り入れられ、実践されています。 しかし、SRE の実践方法はプロダクトの性質や組織の規模、チーム構成によって異なるほか、 文化としても各企業それぞれ異なる解釈をしている場合が多く存在しており有効に活用されていない現状が見られます。とりあえず、流行っているので「インフラエンジニア」という役職名だけ「SRE」にしており実務としては変わらないままみたいなことが聞こえてきて悲しいです。SREはyaml 管理者のことを指していうわけではないです。ここでインフラエンジニアとSREの違いについて三つ紹介したいと思います。1.業務範囲1つ目は、インフラエンジニアは「インフラのみ」が業務範囲であるのに対して、SREは「信頼性を高める活動全て」が業務範囲である点です。インフラエンジニアは、アプリケーション開発チームが開発したサービスが、「高いパフォーマンスで安定的に稼働する」ための環境を構築し運用することが役割です。よって、インフラの構築・運用・改善は行いますが、アプリケーション側には責任は持ちません。これに対してSREは、「サービスの信頼性を高めるための全ての活動」を行います。具体的には、インフラだけでなくアプリケーション側も業務範囲となるため、アプリケーションのプログラムの修正までSREチームにより行われる場合もあります。また、開発や運用のみならず、組織や文化の醸成といった部分まで責任を持って取り組まれる例が多いのも特徴です。2.スキルセット2つ目は、業務範囲の違いからくる、求められるスキルの違いです。インフラエンジニアは「ITインフラに関する知識や技術力」が求められますが、SREはインフラエンジニアが持つ知見に加えて「アプリケーション開発を行う技術力」や「当該アプリケーションに関する深い知見」が求めれます。求められるスキルの違いは、SREチームの構成にも反映されています。SREを提唱したGoogle は、「SREチームの約半分はGoogle の正規のエンジニアで構成される」としています。つまり、SREチームの半数はアプリケーションエンジニア（または経験者）により構成されるべきとしています。そして残りの半数は「Google の正規エンジニア『予備軍』だが、他のメンバーが持っていないスキルがある」ことを条件としています。ここで指す他のメンバーが持っていない具体的なスキルとしては、「UNIXシステムの内部構造」と「ネットワーク（レイヤー1からレイヤー3）」の専門知識であることが圧倒的に多いです。この辺は2011年に発売されたウェブオペレーション――サイト運用管理の実践テクニックなどを参考にしていただけると非常に参考になると思います。言い換えると、SREチームメンバーの半数は、SREからアプリケーション開発チームに異動しても、そのまま業務を行えるレベルで開発力があるメンバー（または直前までアプリケーションチームに所属していたメンバー）で構築されるべきだということです。一般的なインフラエンジニアの多くは、アプリケーション開発チームに異動したとしても、スキルがマッチしないため、その業務遂行が難しいことを考えると、SREチームは技術力の「深さ」だけでなく「広さ」も求められるといえます。3.方法論3つ目は、方法論の有無です。「インフラエンジニアとは、なにをどのようにして行うべきか」という方法論は、企業により大きく異なります。これに対して、SREは明確な方法論があります。具体的には、上記で紹介したGoogle が自社のSREの紹介サイトhttps://sre.goole/において、『Site Reliability Engineering』という「SREの原典」ともいうべき本を無償で公開しています。これらには日本語版や他の言語のものもあるので全世界のSREは、この「SREの原典」を理解した上で、記載されている方法論に従ってSREの業務を行っています。もちろん、企業ごとに「どこまで原典を文字通りに取り入れてSREを行うか」の違いはありますが、大まかな方法論や用語、考え方は全ての企業で共通しています。vs DevOps というおまけWebサービスの信頼性や価値の向上に用いられるアプローチ方法としてSRE（Site Reliability Engineering）というものがあります。システム開発側と運用側の溝を埋めるために生まれたこの手法ですが、従来のDevOpsとはどのような違いがあるのでしょうか。ついでにSREとDevOpsの違いについて見ていきます。SREとDevOpsの違いや関係性を知るには、Googleが提唱している「class SRE implements DevOps」の考えが最も明解でしょう。「class SRE implements DevOps」は、「SREはDevOpsというinterfaceの実装である」という意味を表します。「DevOps = 思想」という定義に対し、それを具体化し実装したものがSREであるという考えです。この辺は概念的な面も多く「実際、どのようにSREを導入すれば良いのだろう？」や「専任のSREチームなしでSRE原則を適用する方法がない」と思う担当者の方も多いかと思いますのでぜひ、紹介した本を読んでみましょう！さいごに現在、以下の3冊がGoogleから出されています。Site Reliability EngineeringThe Site Reliability WorkbookBuilding Secure & Reliable Systemsこのうち、Site Reliability Engineering と The Site Reliability Workbook は日本語版も出版されております。sre.googleそして、日本語での新たなSRE関連書籍が9月3日に発売されました。ちなみにGoogle からではないです。この書籍は大規模なプロダクションシステムの運用において、様々な企業や組織がSREをどのように実践しているかについて紹介している書籍になります。その本のタイトルは『SREの探求――様々な企業におけるサイトリライアビリティエンジニアリングの導入と実践』 です。私は6月にSRE特化型コンサルティング事業を運営するスリーシェイク社に転職して1ヶ月程無職期間を謳歌していたので他にもSRE関連書籍を読みましたがその中でも今回のイベントのタイトルでもある理論と実践について深く書かれているので原典を読んだ上で読むとめちゃくちゃ面白い書籍だと思います。イベントに参加できなくとも信頼性に関わる全てのエンジニアは読んでも良いと思いました。本イベントでインフラエンジニアからSREと名付けられ旅館で迷子になった無垢な子供が救われることを祈ってます。それでは皆様、イベントでお会いしましょう！また、株式会社スリーシェイクではSRE に関するイベントをやっております。登壇者含めて募集しているので皆さん登録よろしくお願いします。3-shake.connpass.comあとがきSRE Gaps「理論と実践からSREを再考する」は本当に良い発表ばかりだったと思う。自分は本当に何もできずにただただ震えてただけですがなんとかいいイベントになったのではないでしょうか？皆様も感想などありましたらハッシュタグ付けてツイートでもしてください！twitter.com2021/9/9 「SRE Gaps 理論と実践からSREを再考する」イベントリポートsreake.com完全なる宣伝になるんですけど「実際、どのようにSREを導入すれば良いのだろう？」や「専任のSREチームなしでSRE原則を適用する方法がない」と思う担当者の方も多いかと思います。弊社は、金融・医療・動画配信・AI・ゲームなど、特に技術力が求められる領域で豊富な経験を持つSREの専門家が集まったチームです。戦略策定から設計・構築・運用、SaaS提供までSREに必要な要素を統合的に提供可能です。もし少しでもSREに興味があるという企業様がいらっしゃいましたら、気軽にお問い合わせください。sreake.com","link":"https://syu-m-5151.hatenablog.com/entry/2021/09/09/142150","isoDate":"2021-09-09T05:21:50.000Z","dateMiliSeconds":1631164910000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"『SREの車窓から』という題で #geekgig に登壇。 ","contentSnippet":"前書き『GeekGig #1 〜Goと私の一年〜』という株式会社Showcase Gigが主催しているイベントでLTで登壇してきました。運営の皆様ありがとうございました。今回の登壇はSREというよりもシステム管理者がGo言語を使うメリットについて言及しました。タイトルはミスリードです。申し訳ございません。しかし、SREやシスアドの仕事は本当に楽しいことばかりではないですが如何に楽しくやっていくかを考えるのは楽しいなと思っています。showcase-gig.connpass.comまた、Modern System Administrationでは特定の言語に縛られてはないのですがシステム管理者がプログラミング言語を習得することについてのいくつかの言及があったので読んでみてください。learning.oreilly.comSREやインフラエンジニアではないのですが個人的には小野 和俊さんの書籍である「その仕事、全部やめてみよう」で言及されている【ラストマン戦略】が好きで、コードが読めたり書けたりするインフラマンというのは重宝されるように思えます。今回の発表はその啓蒙の一つだと思います。www.bookbang.jpインフラとGo言語が混ざったFukuoka.go という最高の勉強会があって直近のものに関しては動画も公開されているのでみておくのも良いかと思います。fukuokago.connpass.comまた、勉強会やコミュニティーに対して定期的に参加することは技術的なモチベーションにも繋がるので発表者になるのも良いかと思います。nulab.com登壇資料の文字起こしと補足各分野や文字に対して響いた時に読んでほしい本がいくつかあるので記載しておきます。 speakerdeck.comQ.今年は私がGo言語で何をしていたか？A.ほぼ全てで Go言語を利用している生業としてはインフラやプラットフォームの構築や開発をしたり、インフラでの便利ツールの作成をやってました。一昨年ぐらいから規模感でPythonやシェルスクリプトを選択するという場面が減りました。私は作業シェルにfishを利用しているのですがこれらの環境に直接影響があるものや自分でライブラリーの開発が必要でそれらが面白くなさそうな場合以外は全てGo言語で実装するようになりました。最近では、個人開発の生産性の観点からも慣れのおかげでGo言語一択になっています。自動化楽したいあわよくば全ての作業は自動化されてほしい。しかし、開発スキルなしでは正確に自動化できず、運用スキルなしでは正しく自動化できません。昨今の運用が抱える自動化はソフトウェアチームの核心である継続的なデプロイや義務である継続的デリバリーだけではありません。しかも、この分野に全ての問題を一挙に解決してくれるカリスマ的なツールは存在せず。いくつかのツールを合わせて利用することになり、Go言語はさまざまなツールに対応しております(ちなみに、シェルスクリプトの知識は大事)。自作可能。クラウドネイティブな世界ではプログラマブルに制御できる範囲がGo言語だと広い関連書籍learning.oreilly.comlearning.oreilly.comlearning.oreilly.comlearning.oreilly.comコラボレーション開発者には運用スキルが必要です。彼らの責任はコードを書くことだけではなく、システムを本番環境にデプロイしてアラートを監視することです。同様に運用者にも開発のスキルが必要です。本番アラートを監視することだけではなく、不具合が起こった時の事象の認識、コードの特定、変更などできれば良い関係になるんではないでしょうか？もし、開発チームと運用チームが同じプログラミング言語で開発していたら素晴らしいと思いませんか(まぁどんな言語でもいいけど)？。それこそ、書籍でしか文字として認識しかしてない開発チームと運用チーム間の双方向のコラボレーションが発揮させれるのではないでしょうか？開発と運用が協力してツールや知識は開発と運用の間のすべてを双方向に生かされて成功は約束された感じがしてきませんか？最近だと、Node.jsやDartなどもっと複数のレイヤーを跨いで協業できる言語も増えています。各チームや組織ごとに最適なものをその場、その場で決めていけば良いと思います。10x.co.jp関連書籍learning.oreilly.comlearning.oreilly.com解像度Goで開発された世界の素晴らしいツールやミドルウェアの実装が読めると自分のチームの開発や運用の役に立つことや自分自身の実力になることも多く良い循環がまわるようになる。現在、SREやインフラエンジニアが使う多くのツールやミドルウェアがGo言語で開発されている。そのため、Go言語特有のエラーやログのメッセージに慣れておくことによって今まで、不思議で意味のなかった文字列が開発者からのメッセージに見えてくるようになって、デバッグ時や問題発生時に非常に役に立ちました。ツールやミドルウェアを使っている時の解像度がグッと上がった気がします。関連書籍learning.oreilly.comlearning.oreilly.com最後にこの発表はハッカー的な意味合いというより自戒や啓蒙などを込めたものでした。年に何回かエモいだけの発表がしたくなるのですが相応に数人が楽しく仕事できればもはやなんでも良いのではないかと思いだしました。次回があればもっと技術に寄った発表をします。でも、転職して3ヶ月で思ったことは本当にhttps://dart.dev/","link":"https://syu-m-5151.hatenablog.com/entry/2021/08/17/114732","isoDate":"2021-08-17T02:47:32.000Z","dateMiliSeconds":1629168452000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"『よみがえるMakefile』という祈り、もしくはJuly Tech Festa 2021 登壇感想","contentSnippet":"概要7月18日に July Tech Festa 2021 が開催されて『よみがえるMakefile』 というタイトルで15:20 から登壇した。今さら聞けないIT技術 というお題だったので今さら聞けないけど… みたいなものが議論の種になればとお題をMakefileに決めました。docs.google.comconnpass はこちらtechfesta.connpass.com文章として大きく離散してるので雑多なまとめになります。タイムアポン完全に終わっていて一連のスライドからGo言語のMakefileを育てていくエピソードを登壇資料から抹消した。正直、資料の中にあったものをシュッとMakefileに落とし込むだけなので特定の言語やツールに依存するかな？と思って排除してしまいました。何故か、40分だと思ってたので20分資料に再編集した。— nwiizo (@nwiizo) 2021年7月18日  資料は、『よみがえるMakefile』 という完全にタイトルで勝ちが確定しているタイトルで発表しました。個人的にはMakefile はCI/CD時代においても最強の可搬性を持つ汎用有能ツールであると思っており、Go言語の利用が広がると共に復活したのかな〜って思うなどしてます。 speakerdeck.comふとした、下書き社内での発表の公開用、かつ発表しながらだったので文章としてどこか雑。インフラエンジニアの作業環境はカオスインフラエンジニアの作業環境はカオスである場合が多い。そのような環境に対応する場合にいくつかのツールが存在する雑な環境を構築するときには自作Shellを整備する  ShellScriptで環境構築を自動化 - Qiita冪等な環境構築を行うときにはAnsibleクラウドに環境を構築するときにはTerraformVMを提供するときにはVagrantコンテナで実施する時はDocker上記のような様々なツールを使うことが多い。上記のツールは環境を構築できるがこれら自体を開発していく上での知見はリポジトリには溜まってなかったりする(どのリンターを使うか？確認方法についてなど)。実際に開発や運用の作業する時には、環境構築だけではないやろ。普通に考えて！運用や開発で得たいくつかのTipsを知見として保存及び継承するために選べる手段として1番最初にパッと出来てしまうのが自作のシェルスクリプトである。自作のシェルスクリプトは応用が効いくが具体的に何をするか一つ目の引数が何で二つ目はあれでみたいな感じで総じて不明瞭であり、それらは把握、保守する人間が必要で、自作のシェルスクリプトはよく古のオーパーツとして発掘されてしまう場合が多い。アーメン🙏🙏🙏Makefile を使う理由としては開発には様々な理由がある。ビルドオプションを指定するのに都合がよく、バージョン情報などを埋め込んだりしやすい。事前にコードジェネレータで書き出す部分がある時はそれらに伴ったコマンドを補完できる。また、特殊に凝ったことも出来ないので古のオーパーツに比べて読める気がする特にGo言語でプロダクトを作る時、Makefile を使ってビルドやツールを指定することが多いです(軽量プログラミング言語では使う機会がグッと少なかった。 )Makefileはたまごっちほど容易く死ぬ。その中でMakefile を育てていけると良い、現場の人が永遠に居てくれていつでも対応できるのがいいけど余裕が無かったり、秘伝のタレ化されてて言語化が難しかったりみたいな事態は発生するよなーって思うなどしてます。この辺の知見の中心になれる会社やエンジニアになりてぇ良い資料たちGNU Make 第3版日本語書籍としてGNU Make 第3版が存在している 正直、文法や意味はこれが最強。アンチパターンについてなど全面同意なので読んで！無料だし！オライリーは最強の書籍ですがそれらをはじめる動機にはなりにくいのかな？って思ってるので俺の登壇にも意味があったはず！kubebuilder で用いられるMakefileは参考になるのでK8sを開発で利用するエンジニアは参考になるのでぜひ！目的に沿ったDocumentation as Codeをいかにして実現していくか / PHPerKaigi 2021Learning GoGo で使う Makefile の育て方Go generate最後にこのブログを共有しながらGistでも貼ってくれた方のものに関しては全て転記しようと思うので皆様何卒よろしくお願いします！個人的に今回の登壇は面白かった。要員としては登壇後の懇親会の gather.town でのフィードバックやごちゃごちゃ感、視聴されてる時の一体感があったような気がします。これらはひとえに運営様の努力だと思いました。","link":"https://syu-m-5151.hatenablog.com/entry/2021/07/26/105940","isoDate":"2021-07-26T01:59:40.000Z","dateMiliSeconds":1627264780000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"Go言語のCLI作成ツールであるcobra のシェル補完の生成があまりにも簡単","contentSnippet":"概要Goでコマンドラインツールを作成する場合の選択肢としてspf13/cobraがあると思いますが、遂にv1.2.0 がリリースされました👏👏👏。大きな機能追加として completion が追加され、大枠の使い方とシェル補完の生成についてこのブログではやっていくメモ書きになってます。それ以上に言及しようと思ったのですが完全に体力不足でーす。やっていくinstallCLIがあるので利用するさまざまな日本語での入門記事がcobra には存在するので詳しくはその辺を参照してください。ちなみに、公式ドキュメントが最高で公式ドキュメントに誤りがあればコントリビューションのチャンス。$ go get -u github.com/spf13/cobraって思ったら雑な備忘録を自分も書いてましたが一切の参考にならんsyu-m-5151.hatenablog.cominitコマンドラインツールの初期化を行う$ cobra --viper=false init --pkg-name github.com/nwiizo/workspace_2021/blog/cobra_generating_shell_completionsmain.go -> cmd.Execute() -> cmd/root.go の rootCmd.Execute() の順番で実行されるのでそれらに準ずるように実装していくのですがその辺も他の最高の入門記事があると思うので参照してください。package mainimport \"github.com/nwiizo/workspace_2021/blog/cobra_generating_shell_completions/cmd\"func main() {    cmd.Execute()実際に実装する際にはcobra/user_guide.md at master · spf13/cobra · GitHubをやっていきましょうadd fizzbuzzサブコマンドの追加をしたい時にはcobra add を実行していく。$ cobra add fizzbuzzfizzbuzz created at /*****/workspace_2021/blog/cobra_generating_shell_completionsサブコマンドが実装されました。$ go run main.go fizzbuzzfizzbuzz calledcmd/fizzbuzz.go が追加されるのでこちらに実装を追加していけば良い。init()->rootCmd.AddCommand(fizzbuzzCmd) は自動生成されるのでfizzbuzzCmdの中身を改修すれば実装できる。そして、実装したのが下記になる。package cmdimport (    \"fmt\"    \"strconv\"    \"github.com/spf13/cobra\")func fizzbuzz(max int) {    for i := 0; i <= max; i++ {        fizz := i%3 == 0        buzz := i%5 == 0        switch {        case fizz && buzz:            fmt.Println(\"fizzbuzz\")        case fizz && !buzz:            fmt.Println(\"fizz\")        case !fizz && buzz:            fmt.Println(\"buzz\")        default:            fmt.Println(i)        }    }}// fizzbuzzCmd represents the fizzbuzz commandvar fizzbuzzCmd = &cobra.Command{    Use:   \"fizzbuzz [int]\",    Short: \"return Fizzbuzz\",    Long: `return Fizzbuzzreturn Fizzbuzz There is no particular reason because it is a suitable sample`,    Run: func(cmd *cobra.Command, args []string) {        fmt.Println(\"fizzbuzz called\")        var m int        m, _ = strconv.Atoi(args[0])        fizzbuzz(m)    },}func init() {    rootCmd.AddCommand(fizzbuzzCmd)}補完のためにcompletion fish を行う補完するためにはgo run main.go で実行するわけにはいかないのでビルドを行う。go build .rootCmd.CompletionOptions.*** 以下に設定を入れれば設定の変更を行うことができます。デフォルトではcompletionは有効なのでfish に読み込ませて、実際にコマンドを実行。$ ./cobra_generating_shell_completions completion fish | source$ ./cobra_generating_shell_completions [tab]completion  (generate the autocompletion script for the specified shell)  fizzbuzz  (return Fizzbuzz)  help  (Help about any command)一瞬で補完させることができ、あまりの素晴らしさに膝から崩れ落ちた。補完させないようにしたり、その他のもろもろに関してはcobra/shell_completions.md at master · spf13/cobra · GitHubを読んでいけば実装できると思う。また、これらの補完と配置はMakefileに書いておけば良いかなって思いましたが、7月18日の JTF2021に登壇するからそう思うだけなのか。。。docs.google.com最後にcobraは入門記事が溢れているがツールとして日々アップデートされているので公式を見るのがいいなぁと自分の過去の記事を読みながら思いました。リリースノートに記載があると思うが変更点や実装などはこの辺を読むと良いのでぜひに〜github.com","link":"https://syu-m-5151.hatenablog.com/entry/2021/07/05/103447","isoDate":"2021-07-05T01:34:47.000Z","dateMiliSeconds":1625448887000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"「Cloud Native Go」の読書感想文","contentSnippet":"近況2021年6月1日に転職したのですが現職では週2回のエンジニアの共有会があるので、そちらで共有する資料を作ったのですがついでに自分のブログにもポストしておこうとおもいます。あと、転職してから周りも優秀でよく褒められるしいい会社に転職できたと思います。syu-m-5151.hatenablog.comここから下が共有会の資料。Cloud Native Goを読んだ。輪読会ではなく共有会としての読書感想文を記載しました。本当に素晴らしい書籍だと思うので共有会を通して皆さんに共有したいと思います。本書にはインフラや実行環境の変化に合わせてアプリケーションにはどのような変化が必要なのか？についてと偉大なるGo言語に関しての言及がされております。嬉しいですね。Cloud Native Golearning.oreilly.comcloud-native-go/examplesgithub.com誰が読むべきか？WebアプリケーションエンジニアDevOps スペシャリストSite Reliability Engineerどんな本なのか？アプリケーションの設計、構築、および展開の方法は変化しています。アプリケーションレイヤーの変化に合わせてどのようなインフラの概念に精通すれば良いのかの指針になるのではないかと思う。特にスリーシェイクは、モダンなインフラレイヤーの技術力を強みにしているので読んで損はないと思う。また、どんな本ではないのか？Kubernetes が学べる本ではない主要なコンテナオーケストレーターであるKubernetesを本番環境で動かすために役立つベストプラクティスやKubernetes の概念が学べる本ではない。Production Kuberneteslearning.oreilly.comKubernetes で必要なYAMLが学べる本ではないGoのアプリケーションをKubernetes に載せるためなどの具体的な課題に対して必要なYAMLや概念を学べる本ではない。Kubernetes Patternslearning.oreilly.comKubernetes を拡張する本ではないKubernetes の機能をGo言語を利用して拡張して課題を解決したいクラスタ運用者や気鋭なアプリケーション開発者のための本ではない。Programming Kuberneteslearning.oreilly.comGolang に入門できる本ではない3章では入門書籍っぽいことが書かれているが入門書としては全然足りない。これを読んでGo言語かける人間はかなり強いと思う。Go言語入門とかみんなのGoとかで勉強しましょう。The Go Programming Languagelearning.oreilly.com改訂2版 みんなのGo言語gihyo.jpマイクロサービスに関する本ではないマイクロサービスのためのシステム分割や設計のレベルから解説する本ではない。マイクロサービスパターン 実践的システムデザインのためのコード解説 - インプレスブックスbook.impress.co.jp各章ざっくりまとめPart I. Going Cloud Native1. What Is a “Cloud Native” Application? (19:33 mins)コンピューティングの歴史とクラウドネイティブなアプリケーションに関する言及2. Why Go Rules the Cloud Native World (19:33 mins)Go言語がいかにクラウドネイティブな価値観において素晴らしい言語なのかについて熱弁していて良いII. Cloud Native Go Constructs (01:09 mins)3. Go Language Foundations (54:03 mins)ざっくり、Go言語に関する入門(本にも限界があるので！)4. Cloud Native Patterns (52:54 mins)1番、最初にみんな大好きな分散コンピューティングの落とし穴に関する言及があるのですが本書には「Services are reliable: services that you depend on can fail at any time」という一文も追加されております。Goでの分散コンピューティングの落とし穴をアプリケーション層で回避する為のパターンに関する言及がある。Context の言及や使い方に関しては入門としても非常に分かりやすいので読んでみてほしいです。5. Building a Cloud Native Service (87:24 mins)net/httpやgorilla/muxを用いてRESTfulな簡単なKey-value アプリの設計、実装する。そして、いろんなパターンや想定をもとに再実行やサーキットブレイカーの実装などを変更追加していってセキュリティとDocker化をやっていく的なストーリーIII. The Cloud Native Attributes (01:09 mins)6. It’s All About Dependability (39:06 mins)信頼性に関する考察や言及やマインドがまとめられた章。実際の実装はない。Twelve-Factor Appの言及があるがそれに基づいた実装の記載などはない。この章は口で言うのは簡単ですが理解するのと実装と運用していくのがめちゃくちゃ難しい。SREの骨子のような章。7. Scalability (46:00 mins)スケーリングにおける様々な条件と要件、スケールアウトが多くの場合で最良の長期戦略であることに関する言及をしました。状態の有無、アプリケーションの状態が本質的に「アンチスケーラビリティ」である理由について。また、様々な実装について効率的なメモリ内キャッシュとメモリリークを回避するためのライブラリなどの紹介を行いモノリシック、マイクロサービス、サーバーレスアーキテクチャなどの紹介について。8. Loose Coupling (66:42 mins)各コンポーネントが密に結合されていることを確認できる方法と、それらの密結合された各コンポーネントを管理する方法についての言及及びメリットデメリットに関する実装を交えた言及。これ、本物の地獄さ混沌さがサンプルだと足りない。9. Resilience (57:30 mins)復元力や回復力に言及している章。インフラエンジニア的にはヘルスチェックのエンドポイントだけやっていれば良いかもしれないですが連鎖的な障害を事前に検証するための実装や検証方法に関して言及する章。実際に最高のSREサービスを目指すなら確実に習得が必要でかなり、いかつい章。10. Manageability (59:48 mins)どのようにしてソフトウェアの保守性を上げるかの問題。前職でもめちゃくちゃ悩んでいた。Go言語で一つ以上のアプリケーションを書いた場合にはしっくりくると思う。実際の実装まで言及できる能力があると良いと思いました(知らんけど)。参照サイトのこれとか分かりやすかった。Manageability（管理性）とは？www.ni.com11. Observability (92:00 mins)可観測性に関する言及をする時にインフラエンジニアにとってログとメトリクスは馴染み深いですがトレーシングに関しては正直、馴染みがありません(ほんまか？)。この章では可観測性についてのおおまかな概要と各種三本柱の実装についてまとめられてます。トレーシングは概要ですら本当にためになるのでぜひ、読んでほしいです。「データは情報ではなく、情報は知識ではなく、知識は理解ではなく、理解は知恵ではありません。」という最初の一文が良すぎます。知恵を共有できる人間になりたいです。このエントリー以降キャッチアップできていないので情報をキャッチアップして知恵にできるOpenTelemetryについての現状まとめ （2020年6月版） - YAMAGUCHI::weblogymotongpoo.hatenablog.comちなみに共有会は明日。輪読会ではなく共有会での資料になります。関心ごとをもっとフォーカスして良いと思いました。さいごに日本語の翻訳本が出てほしい。","link":"https://syu-m-5151.hatenablog.com/entry/2021/06/16/220322","isoDate":"2021-06-16T13:03:22.000Z","dateMiliSeconds":1623848602000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"Golang で正規表現を用いて雑にURLを取得する","contentSnippet":"退職エントリーが予想以上に伸びたので健全な技術ブログを保つため雑な備忘録貼っておきます。敷居はなるべく低くブログを更新を続けるために昼休憩的な雰囲気だけ持ってやっていきます。syu-m-5151.hatenablog.comコード普通に実行すれば値を取得することができます。package mainimport (    \"fmt\"    \"io/ioutil\"    \"net/http\"    \"regexp\")func main() {    // url の指定    url := \"https://3-shake.com/\"    // 正規表現の作成    re, err := regexp.Compile(\"http(.*)://(.*)\")    if err != nil {        return    }    // net/http でのリクエストの発射    resp, _ := http.Get(url)    defer resp.Body.Close()    // []byte でリクエストの中身を取得    byteArray, _ := ioutil.ReadAll(resp.Body)    // 正規表現にあったものを全てlinks に入れる    links := re.FindAllString(string(byteArray), -1)    for i := 0; i < len(links); i++ {        fmt.Println(links[i])    }}実行雑に取得できた。ここから純粋なURLを取得するのは適当に置換してあげれば良いと思います。mac でのコピペはpbcopy が便利なことを知ったので非常に楽です。https://3-shake.com/wp-content/themes/3-shake/assets/images/favicon/favicon.ico\">https://3-shake.com/wp-content/themes/3-shake/assets/images/favicon/favicon_180x180.png\">https://fonts.googleapis.com/css?family=Open+Sans:400,600,700&amp;display=swap\" rel=\"stylesheet\">https://common.3-shake.com/assets/css/3-shake_icons.css\" rel=\"stylesheet\">~~~","link":"https://syu-m-5151.hatenablog.com/entry/2021/06/08/110251","isoDate":"2021-06-08T02:02:51.000Z","dateMiliSeconds":1623117771000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"4年勤めたGMOインターネット株式会社を退職します。","contentSnippet":"はじめに2017年度の新卒で入社したGMOインターネット株式会社を4年2ヶ月で退職いたしました。 様々な人にお世話になったこと自分自身の成長に繋がったことを考えれば本来であれば直接ご挨拶をすべきところ、このような形で失礼致します。しかし、普通に入社できて働けて良かったと思える会社でした。報告勿体ぶるのもどうかと思うので言及しておくと次の職場は既に決まっており、2021年6月1日から、コンテナネイティブに特化したSRE支援事業、データ統合プラットフォーム事業、広告プラットフォーム事業を展開している株式会社スリーシェイクにてソフトウェアエンジニアとして働くことになっております。3-shake.com退職エントリーに書かれていることが本当に真実かは証明できませんし、主観で僕自身が振り返りをしているだけなので暇な方はお付き合い頂き読んでいただければと思います。振り返り人生を振り返るには今しかないので記載してます。会社で何をしていたのか？社内ではGMOインターネット株式会社でのホスティングサービスの開発と運用、お名前.comやいくつかの商材サイトが載っている社内コンテナ基盤の開発と運用、エバンジェリスト業務を行なっていました。いくつかの事例に関しては登壇資料や発表時に言及してましたのでどうぞ。speakerdeck.com入社までバックエンドエンジニアとしてアルバイトをしていたので、就活については、「プログラミング」がたくさんできる会社に入社したいとふわっと考えてました。GMOインターネットに就職することに決めました。入社直後虚無な社会人研修を終えて、私は社内にある多種多様なインフラ全般を扱う部署への配属となりました。入社前はOpenStack を開発する部署への配属を希望していましたが研修やOJT中にCIやIaCがないことに対してイキった発言をしたり、SNSでの一部の言動が社内で問題となり、配属を希望していたチームとは別のチームへの配属となりました。ホスティングサービスの担当となり、Ansible を書いたりGo言語でE2Eテストやインフラテストの自動化を行ったりとかなり充実してました。また、動いているものに対する責任を持つインフラエンジニアとして大切な姿勢を徹底的に叩き込まれて配属に感謝。また、組織の中で仕事をしていく上で技術力だけで通用する割合の低さとカオスな環境に放り込まれて「コード読めばどうにかなる」はFake野郎の言論であることは本当に勉強になりました。辞メンター「やめんたー」と呼びます。別に呼び方はどうでも良いのですが、新卒2年目突入して直ぐに尊敬していたメンターの方が退職されて、その方が持っていた運用と開発業務を一気に巻き取ることになりました。日頃のドキュメント管理の重要性を学びました。この頃から一人でさまざまなタスクを任されたり、オンコールに入ったりと中途半端な仕事が許されなくなってきました。そして、それらを誇りに思ってました。登壇とエバンジェリスト日々の業務の運用・開発は楽しく満足していたのですが、自分の成果を社内の人に話してもイマイチ反応が悪かったので外部の方と話をしたくなって外部登壇をするようになりました。その一環でサイバーセキュリティに関するソフトウェア開発や研究、実験、発表を一年間継続してモノづくりをする長期ハッカソンSecHack365に会社の支援をいただきながら参加したりもしました。本当にありがとうございました。ブランディングが目的でも自身の登壇やブログの執筆、研修講師、イベント準備を業務時間で行えて、それらが個人の評価に繋がるのは嬉しかったです。自分ともう一人のメンバーはITエンジニア本大賞2021の技術書部門とかに選ばれて格の違いというのを見せつけられましたが非常に刺激になりました。転職なぜ、やめるのか？主にこのパートでは誰かの評価が落ちることは覚悟しなければならないのですがあまり言及したくないので退職エントリーの中で一番面白く、皆様が楽しみにしているこのパートは飛ばします。現職のどこに不満を持っていて、そのうち、どこに改善の余地があり、妥協するべきであり、現職でそれが叶わず。次の職では何を期待するのか、についての思慮を重ねた結果としか言えません。そんなに大きな不満もなかったので、いずれ条件が合えば再度、働くのも悪くないか？と思っています。また、普通に対面であればいくらでも喋るので気になる場合は飲みにでも誘って聞いてください。syu-m-5151.hatenablog.comまとめ雑な報告となりましたがこれからも、クラウドネイティブコミュニティ界隈ですし、登壇も続けていこうと思います。また、皆様とは違う形で出会えると思いますが「さよなら、インターネットと思春期」の終わりにです。","link":"https://syu-m-5151.hatenablog.com/entry/2021/05/31/094928","isoDate":"2021-05-31T00:49:28.000Z","dateMiliSeconds":1622422168000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"#JTF2021w にて「Kubernetes Operator の実装と憂鬱」というタイトルで登壇しました。","contentSnippet":"推しテク総選挙 と聞いて 推しの Kubernetes Operator について語る為に登壇しました。資料を作ってたら概要というか想定から事前知識が多すぎて入門記事になってて途中で申し訳ないという感情になりました。この手の基礎がめちゃくちゃ多い場合にはある程度ペルソナ作って喋ることが大事そう… speakerdeck.com本セッションでは kubebuilder を用いた開発を紹介しました。公式ドキュメント は知見の塊のような存在なので何度読んでも良いと思います。また、Kubernetes はインフラ技術の総合格闘技だという言論があると思うが Kubernetes Operator の開発は パンクラチオン だと言えると思うほどに様々な状況があり、それらすべてを実装していくのは長い道のりである丘の半分に行くことにすら大きな労力が必要である。今回はそのような苦悩を発表したかったのですがちょっと至らなかったと思います。気が向いたら追記するが人間の気が向いたら追記するは二度と追記されない。","link":"https://syu-m-5151.hatenablog.com/entry/2021/03/25/094725","isoDate":"2021-03-25T00:47:25.000Z","dateMiliSeconds":1616633245000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"#技育祭 ではじめてでもわかる！コンテナ入門 という講義をしたのですがコンテナ技術以前に何が必要か？","contentSnippet":"概要空腹状態から回復する為に飯を食っても腹が出る人類の不思議について考えていたら夜でした。2021年03月13日 の15:50 から16:50 までの一時間で技育祭というイベントでコンテナの入門講義を行った。以前から事前に情報が共有できない入門講義は罪深いとは思っている。事前知識のある有無や特定のコンテキストへの理解など参加者のスタートラインを確認できないからで講義時間の関係上それらに一線を引いてここからあそこまでの入門講義だと思い込んで講義を行わなければならない。これは地味にシンドイ。講義する側になると予習復習は大事というのが身に染みます。講義の感想にたまにこんな声が聞こえる。コンテナ技術以前の前提知識が必要だと思うこういうことを聞くとオジサンなので彼らの為になるようにXXといういい本があるよ!!! インフラの勉強をするにはこういう名著がある。もちろん、基本的な知識としてネットワーク・ルーティング・スイッチング・ファイアウォール・負荷分散・高可用性・障害復旧・TCPやUDPのサービス・複数のUNIX・複数のウェブサーバー・キャッシュ・データベース・ストレージインフラ・暗号・アルゴリズム・キャパシティ計画立案に精通した人材になることが必要だよと、本格的に這い寄ると彼らは逃げる。なぜなら、彼らはインフラエンジニアになりたいわけではないのだから!!!では、何が必要なのか？ちょっと考えてみる。おまけ資料 speakerdeck.comとその投稿2021年03月13日 #技育祭 にて 15:50 -16:50 より はじめてでもわかる！コンテナ入門 というタイトルで登壇するのでそちらの資料になります。 https://t.co/40b29kdNHZ— nwiizo (@nwiizo) 2021年3月13日  神様ではないので分かりません。エリック・レイモンドはエッセイ「How To Become A Hacker」の中でどんな言語を勉強すべきかを述べている。まずPythonとJavaから始めよ、学ぶのが容易だから。真剣なハッカーはさらに、UnixをハックするためにCを学び、システム管理とCGIスクリプトのためにPerlを学ぶべし。と言っている。 実はこの主張は2007年の公開から少しずつ変更されている。大きな変更として学習すべき言語をJavaからGolang へ変更したことである。まぁ勉強すべき言語や方法はその時期などによって全く異なるということである。ハッカーになろう の基本的なハッキング技術 から何となく環境がちょくちょく変わっているその雰囲気が伝わると嬉しいです。まぁというようにエンジニアが学ぶべきことは年々変わる。素晴らしいハッカー(本来の意味での)たちが C言語を異常に勧めてくるのにはそれなりの理由があるのだが初学者には関係ない。彼らは強力なツールによって簡単に見かけ上の理解に到達できてしまい。それらに押しつぶされた経験がないからである。でも、最初からそんなに潰されたり一歩ずつ進む必要もない。小さな自信を積みながら楽しむやり方だってあるはずです!!!なので、勝手にコンテナ技術以前の前提知識 のオススメを紹介します。シェル芸芸は身を助くということわざがある。一芸を身につけておくと、いざというとき生計を助けることもあるという意味でつまりそういうことです。シェル芸 とは、主にUNIX系オペレーティングシステムにおいて「マウスも使わず、ソースコードも残さず、GUIツールを立ち上げる間もなく、あらゆる調査・計算・テキスト処理を CLI端末へのコマンド入力 一撃で 終わらせること」（USP友の会会長・上田隆一による定義）しかし、これだけは言いたいシェル芸 に助けられるばかりのコンテナ生活です。シェル芸を学ぶと各種コマンドの性質を掴めるようになります。将棋でいうと一手詰め将棋のようなものです()。基本的にコンテナも最終的にはシェルが動作しますのでこれらを学ぶことにあまり損は感じません。こちらの資料は最高なのでぜひ読んでください。    シェル芸初心者によるシェル芸入門  from icchy   www.slideshare.net基本的な構築と自動化...なんだかんだ言ってもインフラの構築経験がなければDockerfile に何を記載すればプロセスが動作するのか分からないと思います。なのでConoHa (宣伝含む) などに好きなミドルウェア(Nginx やApacheなど他多数) をインストールして設定をして様々な設定を遊んでみることも大事かと思います。あとは、自動で構築するのにもある程度慣れた方がいいのでAnsibleないしは類似したツールを使ってみることをオススメします。Dockerfile と違って都度実行結果が異なることも多いのでそこに考慮した書き方は将来的に役に立ってくると思います。Linux の基礎知識LPIC と LinuC に関しては勉強してあまり損しないと思います。レベル1ぐらい持っていても罰は当たりません。最後に好き勝手にいいましたがTwitterで有識者がもっと素晴らしい意見を言ってくれることを祈っています。今日はもう寝ます。おやすみなさい。","link":"https://syu-m-5151.hatenablog.com/entry/2021/03/14/005638","isoDate":"2021-03-13T15:56:38.000Z","dateMiliSeconds":1615650998000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"#CNDO2021 Docker/Kubernetes 開発・運用のためにテスト駆動開発入門 を振り返っていたら朝。","contentSnippet":"もう、朝寝れてないのでついでに明後日の登壇資料の準備をしている。あとは謎の挙動にぶつかってLDAP にキレていたのを Sleep で切り抜けたので明日、聡明な自分が解決してくれることを信じてます。2021年3月11日にCLOUDNATIVE DAYS SPRING 2021 ONLINE に登壇しました(事前収録)。発表では 主に Docker/Kubernetes でのCI 周りのツールの紹介などを行いましたが日々のコンテナイメージのレビューに憑かれた人に向けて多少楽になる一助になればいいと思いました(ほんなこつ???)が時間管理が無限に下手で本当に入門なだけになってしまいました。結局、言いたいことの骨子をまとめてないからこんなことになるんやぞ!!!event.cloudnativedays.jp speakerdeck.comあとは所属組織でCloudNative Days Spring 2021 Onlineにトップスポンサーで協賛をしておりその関連で幕間CMを撮りました。これを見た幼稚園の頃からの幼馴染からインスタ経由でDMが来ました。ソフトウェアエンジニアとして正しい仕事しかしたくないみたいな感情は1年目になくなったのでなんでもやります。登壇資料よりこっちの方が伸びていて複雑な感情になった。やってました… 最後のジャンプだけ見てください…https://t.co/AKJ5UkKkcC https://t.co/TuwMv6cBTk— nwiizo (@nwiizo) 2021年3月11日  今回は登壇にあたって選考などはなく登壇者は動画を送れば終わりである。最初に聞いた時にはどうすんだろ...とギョッとしましたが1日目を終えた時には多様な価値観や発表を聞いてそれらが杞憂であると確信できました。また、公式ブログには以下のような文章があります。『今ならオンラインの特性を生かして、CloudNative Daysをダイナミックな環境でスケーラブルな形に更に進化させられるのではないか？』オンラインでは、誰でも情報を得ることができ、誰もが発信することもできます。オープンな思想のもとに作られたインターネットには境界がありません。そうしたインターネットの成り立ちを思い出し、初心者から達人まで、住んでいる場所を問わず、クラウドネイティブに取り組む人が、・今まで参加者だった人が壁を感じずに発信できる一日目を終えただけですが今回はどの発表も素敵で、非常に多様化していると感じました。しかし、自由であるが故に同じ時間と場所を共有できないコミュニケーションの難しさを感じました、。資料や発表に対する意見を聞けずに悲しい顔をしてます。資料で紹介したツールの紹介DockerDocker  は DockerfileをbuildしてImageを生成して Imageを実行してcontainer が爆誕するという大まかなながれがあるので確認ポイントがDockerfile、DockerImage、実際の環境での3つある。実際の環境での確認は環境にもよるがeBPF とか言い出さなければならないので今回はスコープから外します。hadolintベストプラクティスのDockerイメージを構築するのに役立つよりスマートなDockerfile Linter です。設定ファイルを置くことで特定のルールは無視することができるのでCIにも組み込みやすいと思います。ちなみに、hadolint から指摘されすぎて hadolint 先輩って呼んでます。dockleベストプラクティスのDockerイメージや セキュアなイメージを構築するのに役立つDocker Image Linter です。hadolint との違いは hadolint はDockerfileに対してのLinterなのに対してdockle はDocker Imageに対するLinter であることです。Event を拾う場所、適用する場所が違います。ある程度似たようなことも言ってくるのですが、はじめ易さでいうと hadolint に軍配があるとは思います(独自的偏見)。Trivyコンテナ脆弱性スキャンツールで、コンテナイメージからコンテナの OS パッケージやアプリケーションの依存ライブラリの脆弱性を検出してくれます。レポートの仕方も多様で滅茶苦茶にユーザーの事を考えているツールだと思います。techcrunch.comcontainer-structure-testコンテナ内部でコマンドを実行することで正しい出力やエラーが帰ってくるかどうかや、コンテナ内部のファイルが正しく格納されているかなどの検証を実行できるフレームワークです。Goss っぽいことができます。内部でコマンド実行して結果を確認するだけなんでコマンドで確認できるものは確認できます。ちょっとしたテストだとこれでどうにかなります。ShellCheck資料にないし登壇でも言及してないが非インフラエンジニア も entrypoint.sh などでシェルスクリプトを書く機会が増えると思う。要出典ではあるのだがシェルスクリプトは普通に動いてくれるので想定外の処理 を埋め込んでしまうことが多々ある。そんな時に頼りになるのがshellcheck である。不用意なrm などを諫めてくれたり変数の取り扱いなど良くハマるあれやこれやを指摘してくれる。頼もしいKubernetesKubernetes のマニュフェスト を確認できるツールは大きく分類すると主に3つのカテゴリに分類できます。API Validators とは Kubernetes APIサーバーに対して特定のYAMLマニフェストを検証Built-in checkers とは セキュリティ、ベストプラクティスなどの決まったものの検証を行うCustom validators  とは自らでルールや規約を作成して検証を行う、API Validators と違ってURLの重複チェックなどができないが大体気軽。kubeval Built-in checkerskubeval は、Kubernetes manifest のファイルを検証するために使用され、単純な記述ミスを検知することができます。kubectlには kubectl apply --validate=true --dry-run=true -f manifest.yaml で検証を行う事ができます。kubectlを使った検証方法では実際に対象のmanifestファイルを実行するため権限が必要kube-score Built-in checkerskube-score は、Kubernetes manifest のファイルを分析し、スコアを付けされますセキュリティの推奨事項とベストプラクティスに基づいてチェックされこれらを選択することができます。conftest Custom validatorsconftest は YAML や JSON などの構造化データに対してユニットテストを記述できるツールです。ポリシーはOpen Policy Agent (OPA) で使われているポリシー言語 Rego で記載することができます。柔軟性が高くyaml の確認ができるので様々なツールで利用可能で組織横断で使う設定を決めてしまえるのでかなり、オススメ!!!!!さいごにおわり、。特に振り返りもしてないけど もう眠い。宣伝グループ内でいくつか発表があったので紹介させてください。感想は気が向けばあとから追記します。｢PGマルチペイメントサービス」のマイクロサービス移行計画event.cloudnativedays.jp決済システムにおけるクラウドネイティブへの挑戦event.cloudnativedays.jpインフラ目線でみた、初めてコンテナでサービスをリリースする時のセキュリティポイントevent.cloudnativedays.jp最近、推してる音楽歌詞と歌声があまりにも最高なので聞いてくれ!!!!!!!!年老いて眠くなる死ぬ前になってわかる人は何を恐れて夢み今は生きてることに感心だつまらない感情抱えた今最低なことばかりじゃないyoutu.beOpen Policy Agent Rego Knowledge Sharing Meetup別でOPAのイベントがあったようなのでぜひ、みて欲しくて動画を追加しておきました。youtu.be","link":"https://syu-m-5151.hatenablog.com/entry/2021/03/12/044315","isoDate":"2021-03-11T19:43:15.000Z","dateMiliSeconds":1615491795000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"プログラミングの文法やインフラの知識は増えてやれることは増えたが、作っていくものが良いシステムにならない気がしている社会人エンジニア歴4年目の抱負を言いたい。","contentSnippet":"はじめにプログラミングの文法やウェブオペレーションを中心としたインフラの知識は増えて、障害発生しやすかったり性能が悪いコードや設定の書き方が徐々に分かってきてました。やりたいこと、やりたくないことが自分の中で徐々に理解できるようになってきました。しかし、そうやっていろんなものを作っていく中でそれらが本当に意味のある良いシステムにならない気がしている。ここから先へはどのようなステップアップで進めていけばいいか分からない。10年や20年後にこのブログを読んで振り返りたいと思うので社会人エンジニア歴4年目の抱負を言いたい。誰やねん2017年4月に新卒に技術職としてGMOインターネットに入社して4年目でソフトウェアエンジニアをやっている。インフラ開発と運用が主な業務です。インフラを良くしていくためのサーバーサイドを書いたり、言語はわりとなんでもやっています 。技術広報も拝命しており積極的に外部登壇をしたり社内で勉強会をしたりしている。もともと、入社当初は シェルスクリプト と Python を多用していましたが最近は個人用のツール系でどのような規模のものでもGo で書くようにしてます。Python を使う場面はとても減りました(シェルスクリプトは使いますがGo側からよびだすことが増えました)。個人の生産性の観点でも100行以上である場合にはシェルスクリプトや Python より Go のほうが上にななってきた感じです。IT界隈ではない方も住んでいるシェアハウスに住んでおり筋トレと格闘技が趣味で一昨年までいろんな場所でアマチュアの試合やったりしてました。酒は好きだが直ぐに酔っ払い粗相をするのであまり飲まない。2021年抱負仕事やっていく。一昨年の途中から特定のプロダクトや技術領域に関してリードエンジニアとして関わっているようになったので再度、自分の役割を見直してよりよいシステムを作っていく為に前提、原則、思想、習慣、視点、手法、法則 など普遍的定説的本質的に最適な行動が取れるように様々な分野や媒体での学習を続ける。毎日 GitHub に commit をする。技術の習得には input と output が大事で、input で得たものが output によって定着するのではないかという持論はあります。しかし、社内で作っているものはGithub管理ではないのでどうしたものかと思ってます。あとは、人間っぽいイベントはあきらめたくないなぁ～とも思ってます。79 ㎏ への ダイエット。行き過ぎた筋トレと減量反動による食べ過ぎにより人生で最も大きくなっています。このままでは健康を害する。雑でもいいので読んだ本のブログないしはまとめた内容を文章にする。去年が漫画を除くと 60冊ぐらい読んだのですが明確にOutPut を意識することはなかったのでこちらはやっていきたいと思ってます。お金や人生、将来について計画を立てる。結婚できないなら結婚できないなりに未来についてちゃんと考える。さいごにそんなに未来のことは何もわからないのでこの辺が今年の抱負かと思う。手を動かして勤勉にやっていくための健康を確保しながらinput と output の精度と量を上げて行きたいと思いました。","link":"https://syu-m-5151.hatenablog.com/entry/2021/01/04/162322","isoDate":"2021-01-04T07:23:22.000Z","dateMiliSeconds":1609745002000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"自分用 の2020 年の登壇などの振り返り","contentSnippet":"2020 年の振り返りを雑多に書いていきたいと思います。はじめに2020 年もコンテナ関連のインフラ技術を主軸として運用及び開発を行ってきました。その他にもReact やFlutter の開発ができたりしました。仕事、プライベート、コミュニティにお付き合いあった方々、ありがとうございました。2021 年もnwiizo をどうぞよろしくお願いいたします。2020 年の振り返り1 月12 月から1月までは社内で新規のコンテナ案件が増え、関わるステークホルダーが増えました。それらの方々を対象に同じ資料を使って3回程度社内勉強会をしておりました。 speakerdeck.comほかにもMail Hosting のスパムメールツール関連の 大きな改修とリリースがあったみたいですが記憶がありません。2 月セキュ鉄OWASP九州合同勉強会 にて勉強会を行いました。正直、過去一レベルで失敗しました。正直、よっぽどの理由がない限り環境構築はした状態でハンズオンを始めた方が良いという強い意志を固めました。secsteel.connpass.com speakerdeck.comプライベートではNginx Unit の検証をしており、仕事ではArgoCD Workflow の検証をひたすらにしてました。Workflow やKubernetes scheduling 周りについて興味を持っていて雑なコードが書き殴られていました。3 月外部登壇なしです。GMOインターネットが保持する商材サイトのKubernetes 化を行っていた記憶があります。concourse やDiDの検証や開発者の開発環境を作ったりしてました。4 月Infra Study Meetup #1「Infrastructure as Code」 〜インフラ技術の「これまで」と「これから」を網羅！インフラ勉強会シリーズ第1弾〜 で登壇して、インフラにおけるテストについて雑多にまとめた資料を作りました。ちょうど、この時期は既存環境の温かみのある手順書 + Nagios から Terraform + Ansible + Goss + Prometheus に構成を変えるみたいなことをしており、自分の中でも整理がついたので登壇して、めちゃくちゃよかったです。 speakerdeck.com上記で話したようにいにしえの環境と戦っていたのですが一部コードの修正が必要だったので書き換えとかもやってました。業務後にLSP とかのある環境で開発した時に生産性の向上を感じることができた。LSP最高である。5 月今年もGMO Technology Boot Camp という社内の新卒エンジニアの技術力向上・適性判断を目的とした研修プログラム にてコンテナ技術の講義の担当をした。 speakerdeck.comPrometheus Meetup Tokyo #4 ではPrometheus Operator について少しお話をしました。本当に良くない資料だと思う。 speakerdeck.com5月以降で登壇が完全にリモートになり工夫が足らず完全に登壇に対してネガティブな感情が募っていました。6,7,8 月本当に虚無である。強いていうなら kubebuilder をひたすらにいじっておりました。9 月2018年の修了でOpenFaaS のマルチユーザー対応してました。SecHack365 Beyond に登壇しました。 speakerdeck.com久しぶりに働き方などについて真剣に考えた。NO HARD WORK!　無駄ゼロで結果を出すぼくらの働き方 を読んだりその仕事、全部やめてみよう――１％の本質をつかむ「シンプルな考え方」 などを読んだり自分自身が4年間社会人をやった感想みたいなことを喋って時間が足りなくなった。自作 Kubernetes Operator による監視/通知自動化周りでリリースをこなした。10月Kubernetes Meetup Tokyo #35 にて Kubernetes API との邂逅 あるいは kubewebhook 入門 というタイトルで発表しました。kubewebhook は最高なので…。 speakerdeck.com仕事はクラスター周りのトラブルシューティングをひたすらにやっていた。11月Kubebuilder を用いて単純なCRUD API のOperator を実装していく上で必要な知識や様々な概念の説明 をしたかったのですが要約する場所が全然分からず自分の力不足を感じました。 speakerdeck.com12月はじめてでもわかる！コンテナ入門 - 社内研修研修公開しちゃいます - というイベントで講師しました。もう飽きたので終わり speakerdeck.com","link":"https://syu-m-5151.hatenablog.com/entry/2020/12/30/181640","isoDate":"2020-12-30T09:16:40.000Z","dateMiliSeconds":1609319800000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"Kubernetes で readOnlyRootFilesystem の設定がTrue になってないのをデプロイする前にConftest したい","contentSnippet":"概要Kubernetes ではポッドセキュリティポリシーにより、ポッドの作成と更新のきめ細かい承認が可能になります。また、Pod に直接設定するだけではなくRBAC を利用したデフォルトの設定を行うことができたり、OPAのgatekeepeを利用することで開発者は意識することなくセキュリティの設定を行うことができます。こちらにはreadOnlyRootFilesystemというオプションがあり、このオプションを有効にするとコンテナ内がreadonlyになるためセキュリティの施策の１つとして有効にすべき場面が多いと思います。こちらは有用ですが yamlを作成していざ、本番へというタイミングでしか分からないのは流石にしんどいです。そのため、デプロイの前にこれはテストをすることは無駄ではありません。readOnlyRootFilesystem をやってみるapiVersion: v1kind: Podmetadata:  name: read-onlyroot-filesystemspec:  containers:  - name: centos7    image: centos:7    command: [ \"sh\", \"-c\", \"sleep 1h\" ]    securityContext:      readOnlyRootFilesystem: true  - name: centos8    image: centos:8    command: [ \"sh\", \"-c\", \"sleep 1h\" ]Podの作成kubectl apply -f readonly_pod.yaml pod/read-onlyroot-filesystem createdkubectl get pod NAME                       READY   STATUS    RESTARTS   AGEread-onlyroot-filesystem   2/2     Running   0          15mファイルのWrite 及び Read/Write を実施するkubectl exec -it read-onlyroot-filesystem -c centos7 touch testfilekubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl kubectl exec [POD] -- [COMMAND] instead.touch: cannot touch 'testfile': Read-only file systemcommand terminated with exit code 1$ kubectl exec -it read-onlyroot-filesystem -c centos7 ls   kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl kubectl exec [POD] -- [COMMAND] instead.anaconda-post.log  bin  dev  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var$ kubectl exec -it read-onlyroot-filesystem -c centos7 pwd       kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl kubectl exec [POD] -- [COMMAND] instead./テストするgatekeepe や ポッドセキュリティポリシー をユーザーやNamespace 単位で実施することにより開発者は意識することなくセキュリティの設定を行うことができますがyamlを作成していざ、本番へというタイミングでしか分からないので流石にめんどくさいです。そのため、デプロイの前にこれはテストをすることは無駄ではありません。conftest は YAML や JSON などの構造化データに対してユニットテストを記述できるツールです。install と実行$ wget https://github.com/open-policy-agent/conftest/releases/download/v0.21.0/conftest_0.21.0_Linux_x86_64.tar.gz$ tar xzf conftest_0.21.0_Linux_x86_64.tar.gz$ sudo mv conftest /usr/local/bininput.spec.containers.securityContext.readOnlyRootFilesystem が正であればこれで通ります。package maindeny[msg] {  input.kind == \"Pod\"  input.spec.containers.securityContext.readOnlyRootFilesystem  msg := \"Containers must not write\"}Pass しました$ conftest test readonly_pod.yaml1 test, 1 passed, 0 warnings, 0 failures, 0 exceptions書き込みは許可したい場合はnot input.spec.containers.securityContext.readOnlyRootFilesystemのような設定をぶちこんであげれば良いです。package maindeny[msg] {  input.kind == \"Pod\"  not input.spec.containers.securityContext.readOnlyRootFilesystem  msg := \"Containers must write\"}そして、実行すると、想定通りに失敗します。事前に分かるというメリットは以外に多くありますのでぜひ、皆さんの環境でも試してみてはいかがでしょうか？conftest test readonly_pod.yamlFAIL - readonly_pod.yaml - Containers must write1 test, 0 passed, 0 warnings, 1 failure, 0 exceptions","link":"https://syu-m-5151.hatenablog.com/entry/2020/11/25/185946","isoDate":"2020-11-25T09:59:46.000Z","dateMiliSeconds":1606298386000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"Kubebuilder の前にCRDへ入門する #osc20fk Kubernetes Operator の直観 予稿 (2)","contentSnippet":"2020年11月28日(土) に Open Source Conference 2020 Online/Fukuoka でKubernetes Operator の直観 というタイトルでセッションを行うための予稿である。前回、大切な考え方や説明や概念をすっ飛ばしていろいろ進めてきました。今回も特段説明する予定がないのでこの辺の知識が必要な場合には 実践入門 Kubernetesカスタムコントローラーへの道 や Programming Kubernetes: Developing Cloud-Native Applications などを読むとよいです。はじめにリソースとは何らかのオブジェクトを概念です。皆さんがよく知る概念としてDeploymentやPodsがリソースです。リソースはKubernetes APIを持ち、実際に配備されているオブジェクトとしてのPodsがリソースとしてのPodsに格納されます。もう一つの大切な概念としてオブジェクトというものがあります。オブジェクトとは持続的なエンティティのことで、Kubernetesクラスターの状態を定義します。オブジェクトとはデプロイされた実際のPodsやServiceのことです。Custom Resources は、Kubernetesの特別なリソースです。Kubernetesを通常の方法で使用する場合、そのようなリソースを作成する必要はありません。したがって、これは多くのユーザーにとってそれほど重要ではありませんがKubebuilder への入門に関しては重要です。Custom Resources は、特別なリソースではありますが特殊なことはできません。CRD (CustomResourceDefinitions) はテーブル CR (CustomResource) はオブジェクトで「リンゴ」のような各レコードのような関係になっていきます。ちなみにKubebuilderではcontroller-genというツールを利用して、Goで記述したstructからCRDを生成する方式を採用しています。controller-toolsはコントローラーを構築するためのgoライブラリのセットで controller-runtime は、コントローラーをビルドするためのgoライブラリのセットです。やっていくKind でcluster を作成するCluster を作成するために$ kind create cluster --name crdCreating cluster \"crd\" ... ✓ Ensuring node image (kindest/node:v1.18.2) 🖼 ✓ Preparing nodes 📦 ✓ Writing configuration 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾Set kubectl context to \"kind-crd\"You can now use your cluster with:kubectl cluster-info --context kind-crdThanks for using kind! 😊## clusters の確認$ kind get clusterscrd# コンフィグの生成$ kind get kubeconfig --name crd  > kubeconfig.yamlCRD をデプロイするsample-controller/artifacts/examples at master · kubernetes/sample-controller · GitHub に出てきた最も単純なサンプルがこちらです。apiVersion: apiextensions.k8s.io/v1beta1kind: CustomResourceDefinitionmetadata:  name: foos.samplecontroller.k8s.iospec:  group: samplecontroller.k8s.io  version: v1alpha1  names:    kind: Foo    plural: foos  scope: Namespacedそれに対応するリソースがこちらです。apiVersion: samplecontroller.k8s.io/v1alpha1kind: Foometadata:  name: example-foospec:  deploymentName: example-foo  replicas: 1実際にデプロイしてみましょうkubectl apply -f foo_crd.yamlcustomresourcedefinition.apiextensions.k8s.io/foos.samplecontroller.k8s.io createdkubectl apply -f foo_example.yaml foo.samplecontroller.k8s.io/example-foo created確認してみる$ kubectl get fooNAME          AGEexample-foo   10s最後に最小でデプロイして確認できることが分かりましたがAPIもなければ特に設定も行っていないのでetcd に情報が保存されているだけの状態になります。その他にもさまざまな機能がある。Advanced topics で いくつか説明している。詳しいことは公式ドキュメントを読んで欲しいです。なぜなら、これはKubebuilder の為のCRD入門なので!!!!! Finalizer はカスタムオブジェクト削除前の処理を定義します。Finalizerを定義することで、カスタムオブジェクトをkubectl deleteした時、実際に削除される前に実行すべき処理を定義できます。Validationはカスタムオブジェクトの設定値が要件を満たしているか、妥当性を確認することバリデーションをかけてチェックを行えます。Printerはkubectl get する際に表示するパラメータを定義することが可能です。SubresourceはCRDのAPIエンドポイントのサブリソースとして機能します。この辺は流石に知っておくと良いと思います。kubebuilder create apiコマンドで生成されたapi/v1/<**>_types.goを見てみると、XxSpec, XxStatus, Xx, XxListというような構造体が定義されており、// +kubebuilder:から始まるマーカーコメントが付与されています。 make manifestsを実行するとcontroller-genなどのツールによりこれらの構造体とマーカーを頼りにCRDの生成をおこないます。make manifests で生成されたcontroller-genなどから生成された CRD を読んでもマジでなんのことか意味分からないので先にCRD の学習をすることを強く勧めます","link":"https://syu-m-5151.hatenablog.com/entry/2020/11/18/150126","isoDate":"2020-11-18T06:01:26.000Z","dateMiliSeconds":1605679286000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"kubeval でマニフェストを確認できるので質問する前に一度、確認お願いします。","contentSnippet":"最初に kubeval というツールを使って、Kubernetes のマニフェストをチェックできます。kubeval は、Kubernetes manifest のファイルを検証するために使用され、単純な記述ミスを検知することができます。yaml の記述ミスは目視だと普通に見逃すことが多いと思います。なので、開発ワークフローの一部やCI、ローカルで使用することで様々なやり取りを減らして開発を円滑に進めることができます。この記事ではkubeval の基本的な使い方を説明することになりますがこれを読み終えた後に皆様がkubeval をインストールしてくれるのを切に願っております。使い方としては標準入力として与えるかファイルを引数で与える形になります。kubeval <file> [file...] [flags]一般的なNginx のDeploymentであるnginx-deployment.yamlを用意します。apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2kind: Deploymentmetadata:  name: nginx-deploymentspec:  selector:    matchLabels:      app: nginx  replicas: 2 # tells deployment to run 2 pods matching the template  template:    metadata:      labels:        app: nginx    spec:      containers:      - name: nginx        image: nginx:1.14.2        ports:        - containerPort: 80kubeval インストールInstalling Kubeval よりインストールを行うことが出来ます(関係ないですがGoReleaser 滅茶苦茶便利ですよね～）。kubeval 実行kubeval はファイルをコマンドへ直接渡すか、標準入力として与えてあげることで実行できます。標準入力で渡してあげればよいということはkustomizeやhelm でファイルを生成する場合でも簡単にCIなどに載せたりチェックすることができるということです。嬉しいですよね!?!?!??$ cat nginx-deployment.yaml | kubevalPASS - stdin contains a valid Deployment (nginx-deployment)$ kubeval nginx-deployment.yaml PASS - nginx-deployment.yaml contains a valid Deployment (nginx-deployment)例えば、replicas: 2 の前に無駄な空白がある下記のような場合があります。spec:  selector:    matchLabels:      app: nginx   replicas: 2 # tells deployment to run 2 pods matching the template  template:実行してみると構文エラーが検出できていると思います。kubeval err-nginx-deployment.yamlERR  - Failed to decode YAML from err-nginx-deployment.yaml: error converting YAML to JSON: yaml: line 8: did not find expected key次のパターンでは 数字型を与えているはずの部分に下記のように数字を与えてしまっています。    spec:      containers:      - name: nginx        image: nginx:1.14.2        ports:        - containerPort: \"80\"実行してみるとワーニングが発生しており型errorを検出できていると思います。kubeval miss-nginx-deployment.yamlWARN - miss-nginx-deployment.yaml contains an invalid Deployment (miss-nginx-deployment) - spec.template.spec.containers.0.ports.0.containerPort: Invalid type. Expected: integer, given: stringちなみにkubevalはKubernetesAPIから生成されたスキーマに依存しております。そのため、CRDのリソースは検証できません。--ignore-missing-schemas や --skip-kinds で検証を飛ばすことが推奨されております。vim で使うvim でプログラムを実行する場合にはノーマルモードで!をつけることで外部プログラムを実行できる 例としては:!ls でlsを実行できます。ちなみに、現在編集中ファイルの省略記号である%を使うことで、編集中へのプログラム実行も簡単にできるので下記のコマンドで簡単に編集中の環境もチェックすることができます。:!kubeval %最後にkubeval でマニフェストを確認できるので質問する前に一度、確認お願いします。お互いの為に。","link":"https://syu-m-5151.hatenablog.com/entry/2020/11/08/213821","isoDate":"2020-11-08T12:38:21.000Z","dateMiliSeconds":1604839101000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"Kubebuilder と触れ合う #osc20fk Kubernetes Operator の直観 予稿 (1) ","contentSnippet":"2020年11月28日(土) に Open Source Conference 2020 Online/Fukuoka でKubernetes Operator の直観 というタイトルでセッションを行う。レベルは入門編として対象者はKubernetes Operator の開発運用を検討している人で前提知識についてはKubernetes に関する書籍を何となく理解できる方としているがそんなレベル感で聞いてくれる酔狂な人間はなかなかいないと思う。そのため、何回かに分けて Kubebuilder やその周辺知識について解説したいと思いますが図も何もない中でこんな文章読まされても意味ないので絶対に読んでほしいとかもありません。ちなみに、書籍であれば 実践入門 Kubernetesカスタムコントローラーへの道 や Programming Kubernetes: Developing Cloud-Native Applications などを読むとよいのでオススメです。はじめにKubernetesには、分散システムを構築、運用に必要な機能を提供する多くのAPIがあります。しかし、これらは意図的に汎用であり、およそ80％のユースケースを対象としています。Kubernetesに存在するアドオンと拡張機能の豊富なエコシステムを利用すると、重要な新機能を追加し、クラスタを拡張することができる。Kubernetes の動作をカスタマイズするには、Continuous Integration 側ではconftest や kubeval などを用いてポリシーを定義し、マニフェストが定義したポリシーを満たしているかテストする方法がある。これは導入コストは低いが強制力も弱い。Kubernetes が提供しているAPI機能を拡張する為の方法としてgatekeeper や Pod Security Policiesを用いることで同様にAPI Server 側のチェックを行う事ができる。導入コストはそここで強制力がある。しかし、柔軟性に欠ける。それ以外にもKubernetesが提供しているAPI機能を拡張する為の方法として主に次のモノが挙げられる。Admission WebhookCRDAPI Aggregation今回のお題はAdmission Webhook や CRD の開発を円滑に進める為のツールの Kubebuilder を用いて実際に作成した何もしないリソースをデプロイしたいと思います。Kubebuilder についてKubernetes 向けのAdmission Webhook は比較的にシュッと開発することができますがCustom Resourcesは、Kubernetesの設計コンセプトを正しく理解する必要があり、たくさんのマニフェストを記述する必要もあるため、非常に難しいです。Kubebuilder  では、controller-tools,controller-runtimeを用いて抽象化したライブラリと、マニフェストを自動生成するツール群を提供することで、Kubernetesの設計コンセプトを完璧に正しく理解する必要もたくさんのマニュフェスト群も書かずに簡単にカスタムコントローラを開発できます。Kubebuilder は、API を構築するための以下のような開発者のワークフローを容易にすることを試みている。1.新しいプロジェクトディレクトリの作成2.1つ以上のリソースAPIをCRDとして作成し、リソースにフィールドを追加。3.コントローラにリコンサイルループを実装し、追加リソースを見る4.クラスタに対して実行してテストする（CRDをセルフインストールし、コントローラを自動的に起動する5.新しいフィールドとビジネスロジックをテストするために、ブートストラップされた統合テストを更新。6.提供されたDockerfileからコンテナをビルドして公開Kubebuilder の背景や設計思想については [KubeBuilder Design Principles] を読んでいただければと思います。ちなみに本来であれば、この後にKubernetes のアーキテクチャやAPI-Server について及びあるべき姿と実際の状態を比較して実際の状態をあるべき状態へと近づける Reconciliation Loop と呼ばれる処理についてやCRD についても説明した方が良いのですが ゼロから始めるKubernetes Controller などの資料があるのでそちらを参照下さい。環境構築Quick Start - The Kubebuilder Book を参考にしていただければ基本的に環境構築は問題ないと思います。kubebuilder installos=$(go env GOOS)arch=$(go env GOARCH)# download kubebuilder and extract it to tmp# curl -L https://go.kubebuilder.io/dl/2.3.1/linux/amd64 | tar -xz -C /tmp/curl -L https://go.kubebuilder.io/dl/2.3.1/${os}/${arch} | tar -xz -C /tmp/# move to a long-term location and put it on your path# (you'll need to set the KUBEBUILDER_ASSETS env var if you put it somewhere else)sudo mv /tmp/kubebuilder_2.3.1_${os}_${arch} /usr/local/kubebuilderexport PATH=$PATH:/usr/local/kubebuilder/binはい、インストールできたと思います。kubebuilder --helpDevelopment kit for building Kubernetes extensions and tools.Provides libraries and tools to create new projects, APIs and controllers.Includes tools for packaging artifacts into an installer container.Typical project lifecycle:- initialize a project:  kubebuilder init --domain example.com --license apache2 --owner \"The Kubernetes authors\"- create one or more a new resource APIs and add your code to them:  kubebuilder create api --group <group> --version <version> --kind <Kind>Create resource will prompt the user for if it should scaffold the Resource and / or Controller. To onlyscaffold a Controller for an existing Resource, select \"n\" for Resource. To only definethe schema for a Resource without writing a Controller, select \"n\" for Controller.After the scaffold is written, api will run make on the project.Usage:  kubebuilder [command]Available Commands:  create      Scaffold a Kubernetes API or webhook.  edit        This command will edit the project configuration  help        Help about any command  init        Initialize a new project  version     Print the kubebuilder versionFlags:  -h, --help   help for kubebuilderUse \"kubebuilder [command] --help\" for more information about a command.kubebuilder initkubebuilder init で プロジェクトを作成したいと思います。* --domain : APIグループのドメインを指定* --license : ソフトウェアライセンスを選択* --owner : このソフトウェアのオーナーを指定$ go mod init kubebuilder-test-controller $ kubebuilder init --domain nwiizo.dev --license apache2 --owner nwiizoWriting scaffold for you to edit...Get controller runtime:$ go get sigs.k8s.io/controller-runtime@v0.5.0Update go.mod:$ go mod tidyRunning make:$ make/home/smotouchi/go/bin/controller-gen object:headerFile=\"hack/boilerplate.go.txt\" paths=\"./...\"go fmt ./...go vet ./...go build -o bin/manager main.goNext: define a resource with:$ kubebuilder create api成果物の確認$ tree.├── Dockerfile├── Makefile├── PROJECT├── README.md├── bin│   └── manager├── config│   ├── certmanager│   │   ├── certificate.yaml│   │   ├── kustomization.yaml│   │   └── kustomizeconfig.yaml│   ├── default│   │   ├── kustomization.yaml│   │   ├── manager_auth_proxy_patch.yaml│   │   ├── manager_webhook_patch.yaml│   │   └── webhookcainjection_patch.yaml│   ├── manager│   │   ├── kustomization.yaml│   │   └── manager.yaml│   ├── prometheus│   │   ├── kustomization.yaml│   │   └── monitor.yaml│   ├── rbac│   │   ├── auth_proxy_client_clusterrole.yaml│   │   ├── auth_proxy_role.yaml│   │   ├── auth_proxy_role_binding.yaml│   │   ├── auth_proxy_service.yaml│   │   ├── kustomization.yaml│   │   ├── leader_election_role.yaml│   │   ├── leader_election_role_binding.yaml│   │   └── role_binding.yaml│   └── webhook│       ├── kustomization.yaml│       ├── kustomizeconfig.yaml│       └── service.yaml├── go.mod├── go.sum├── hack│   └── boilerplate.go.txt└── main.gokubebuilder create apiAPI作成してリソースの定義するために  kubebuilder create api  を実行してきます。みんな大好き --help を使っていけば様々な情報が得られるので割愛しますが素晴らしい情報がたくさんあるのでよろしくお願いします。#$ kubebuilder create api --group ship --version v1beta1 --kind Frigate Create Resource [y/n]yCreate Controller [y/n]yWriting scaffold for you to edit...api/v1beta1/frigate_types.gocontrollers/frigate_controller.goRunning make:$ make/home/smotouchi/go/bin/controller-gen object:headerFile=\"hack/boilerplate.go.txt\" paths=\"./...\"go fmt ./...go vet ./...go build -o bin/manager main.go実行するための環境構築 Kind でのcluster の構築手元のconfigから接続出来るKubernetes Cluster に入れていくためのCluster 作成しました。$ kind create cluster --name kubebuilder-test-controller # Cluster の作成 出力省略$ kind get clusters # Cluster の確認kubebuilder-test-controller$ kind get kubeconfig --name kubebuilder-test-controller  > kubeconfig.yamlmake installcontroller-gen を実行してCRD を実行します。$ make install~/go/bin/controller-gen \"crd:trivialVersions=true\" rbac:roleName=manager-role webhook paths=\"./...\" output:crd:artifacts:config=config/crd/baseskustomize build config/crd | kubectl apply -f -customresourcedefinition.apiextensions.k8s.io/frigates.ship.nwiizo.dev createdmake runmake run を実行したらcontroller のビルドと実行が出来ており、先程作成したCRDのcontrollerとして機能していると思います。$ make run/home/smotouchi/go/bin/controller-gen object:headerFile=\"hack/boilerplate.go.txt\" paths=\"./...\"go fmt ./...go vet ./.../home/smotouchi/go/bin/controller-gen \"crd:trivialVersions=true\" rbac:roleName=manager-role webhook paths=\"./...\" output:crd:artifacts:config=config/crd/basesgo run ./main.go2020-11-02T06:45:33.772Z        INFO    controller-runtime.metrics      metrics server is starting to listen    {\"addr\": \":8080\"}2020-11-02T06:45:33.773Z        INFO    setup   starting manager2020-11-02T06:45:33.773Z        INFO    controller-runtime.manager      starting metrics server {\"path\": \"/metrics\"}2020-11-02T06:45:33.777Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"frigate\", \"source\": \"kind source: /, Kind=\"}2020-11-02T06:45:33.877Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"frigate\"}2020-11-02T06:45:33.877Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"frigate\", \"worker count\": 1}kubectl apply -f config/samples/ship_v1beta1_frigate.yamlkubectl apply -f config/samples/ship_v1beta1_frigate.yaml を実行したらcontroller のようなログが出てきていると思います。2020-11-02T06:49:54.186Z        DEBUG   controller-runtime.controller   Successfully Reconciled {\"controller\": \"frigate\", \"request\": \"default/frigate-sample\"}そして、frigate-sample  がデプロイされていることを確認できると思います。今回は何もしないリソースをデプロイしたので何もしません。何もしていない controllerを見ていきましょう$ kubectl get crd/frigates.ship.nwiizo.devNAME                       CREATED ATfrigates.ship.nwiizo.dev   2020-11-02T06:42:36Z$ kubectl get frigate NAME             AGEfrigate-sample   6m23s$ kubectl get frigate/frigate-sample -o yaml apiVersion: ship.nwiizo.dev/v1beta1kind: Frigatemetadata:  annotations:    kubectl.kubernetes.io/last-applied-configuration: |      {\"apiVersion\":\"ship.nwiizo.dev/v1beta1\",\"kind\":\"Frigate\",\"metadata\":{\"annotations\":{},\"name\":\"frigate-sample\",\"namespace\":\"default\"},\"spec\":{\"foo\":\"bar\"}}  creationTimestamp: \"2020-11-02T06:58:32Z\"  generation: 1  managedFields:  - apiVersion: ship.nwiizo.dev/v1beta1    fieldsType: FieldsV1    fieldsV1:      f:metadata:        f:annotations:          .: {}          f:kubectl.kubernetes.io/last-applied-configuration: {}      f:spec:        .: {}        f:foo: {}    manager: kubectl    operation: Update    time: \"2020-11-02T06:58:32Z\"  name: frigate-sample  namespace: default  resourceVersion: \"3165\"  selfLink: /apis/ship.nwiizo.dev/v1beta1/namespaces/default/frigates/frigate-sample  uid: 2daa801b-d728-4de8-b7f1-ae56ad5eab61spec:  foo: barこれで、なにもしないリソースをデプロイできたと思います。controllers/frigate_controller.go最後に controllers/frigate_controller.go の Reconcile の中は現在このようになっていると思います。各種 ロジックなどを実装したい場合にはReconcile の中に諸々を書いてき再度ビルドしなおせば基本的にはOkです。 37 // +kubebuilder:rbac:groups=ship.nwiizo.dev,resources=frigates,verbs=get;list;watch;create;update;patch;delete 38 // +kubebuilder:rbac:groups=ship.nwiizo.dev,resources=frigates/status,verbs=get;update;patch 39 40 func (r *FrigateReconciler) Reconcile(req ctrl.Request) (ctrl.Result, error) { 41 ▸---_ = context.Background() 42 ▸---_ = r.Log.WithValues(\"frigate\", req.NamespacedName) 43 44 ▸---// your logic here 45 46 ▸---return ctrl.Result{}, nil 47 }最後に次回のブログではReconcile の実装に触れてみたいと思います。","link":"https://syu-m-5151.hatenablog.com/entry/2020/11/02/170525","isoDate":"2020-11-02T08:05:25.000Z","dateMiliSeconds":1604304325000,"authorName":"nwiizo","authorId":"nwiizo"}]},"__N_SSG":true}